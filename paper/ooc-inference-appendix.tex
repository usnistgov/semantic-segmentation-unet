%\def\year{2020}\relax
%
%\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
%\usepackage{aaai20}  % DO NOT CHANGE THIS
%\usepackage{times}  % DO NOT CHANGE THIS
%\usepackage{helvet} % DO NOT CHANGE THIS
%\usepackage{courier}  % DO NOT CHANGE THIS
%\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
%\usepackage{graphicx} % DO NOT CHANGE THIS
%\urlstyle{rm} % DO NOT CHANGE THIS
%\def\UrlFont{\rm}  % DO NOT CHANGE THIS
%\usepackage{graphicx}  % DO NOT CHANGE THIS
%\frenchspacing  % DO NOT CHANGE THIS
%\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
%\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS
%
%\usepackage{amsmath}
%%\usepackage{amssymb}
%\usepackage[binary-units]{siunitx}
%
%
%
%\begin{document}


\section{Appendix A: Derivation of Simplified Halo Formula}

Let us assume that in the entire U-Net architecture the kernel size is constant $k_{c} = k = const$ and each level has the same number of convolutional layers on both decoder and encoder sides $n_{l} = n = const$. If these constraints are satisfied, then the general formula for determining halo can be simplified as follows:

\begin{equation}
\begin{aligned}
Halo = \sum_{c=1}^{N} 2^{l_c} \lfloor \frac{k_c}{2} \rfloor \\
= \lfloor \frac{k}{2} \rfloor \times \sum_{c=1}^{N} 2^{l_c} \\
= \lfloor \frac{k}{2} \rfloor \times ( 2 \times \sum_{m=0}^{M-1} (2^{m} \times n) + 2^{M} \times n )  \\
= \lfloor \frac{k}{2} \rfloor \times n \times ( 2 \times \frac{1 \times (1 - 2^M) }{ 1- 2} ) + 2^{M} )  \\
= \lfloor \frac{k}{2} \rfloor \times n \times (3 \times 2^{M} - 2) 
\end{aligned}
\label{eq:halo2}
\end{equation}

where $M$ is the maximum U-Net level $M = \max_{\forall c}\{ l_{c} \}$ .

For U-Net architecture, the parameters are $k = 3$, $n=2$ and $M=4$, and the equation yields $Halo=92$.

For DenseNet architecture, the parameters are $k = 3$, $n=4$ and $M=5$, and the equation yields $Halo=376$. This value differs by one from the value computed according to Equation 1 because the DenseNet has asymmetry between the first encoder layer with a kernel size $k = 3$ and the last decoder layer with a kernel size $k = 1$.   



\section{Appendix B: Example U-Net Halo Calculation}

Following Equation \ref{eq:halo} for U-Net results in a required halo of 92 pixels in order to provide the network with all of the local context it needs to predict the outputs correctly. With $k = 3$, $\lfloor \frac{k_c}{2} \rfloor$ reduces to $\lfloor \frac{3}{2} \rfloor = 1$. The halo computation for U-Net thus reduces to a sum of $2^{l_c}$ terms for each convolutional layer encountered along the longest path from input to output as shown in Equation \ref{eq:unet-halo}. 

\begin{equation}
Halo = \sum_{c=0}^{17} 2^{l_c}
\label{eq:unet-halo}
\end{equation}

By substituting the level numbers for each convolutional layer from 0 to 17 as shown in Equation \ref{eq:unet-halo-num}, one obtains the minimum halo value of 92 pixels. 

\begin{equation}
\begin{small}
\begin{aligned} 
l_c = \{0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 3, 3, 2, 2, 1, 1, 0, 0\} \\
92 = 2^0 + 2^0 + 2^1 + 2^1 + 2^2 + 2^2 + 2^3 + ...
\end{aligned}
\end{small}
\label{eq:unet-halo-num}
\end{equation}

Similarly, according to Equation \ref{eq:halo2}, the calculation simplifies to:
\begin{equation}
\begin{aligned} 
M=\max_{\forall c} { l_c } = 4\\
k_c = k = 1  \\
n_l = n = 2 \\
92 = 1 \times 2 \times (3 \times 2^4 - 2)
\end{aligned}
\label{eq:unet-halo-num2}
\end{equation}


\section{Appendix C: Error Metrics}

\begin{table}[h!]
	\centering
	\caption{UnTrained U-Net Error Metrics for Tile Size 1024}
	\label{tab:tile_size_1024}
	\begin{tabular}{r|r|r|r|r|r|r}
		TileSize & ZoR & Halo & RMSE    & ME & NME & RelativeRuntime \\ 
		\hline
		3584 & n/a & n/a & 0.0 & 0.0 & 0.0 & 1.0 \\
		1024 & 1024 & 0 & $\num{2.36e-04}$ & 21856.9 & $\num{1.7e-03}$ & 1.08 \\
		1024 & 992 & 16 & $\num{1.05e-06}$ & 234.8 & $\num{1.8e-05}$ & 1.23 \\
		1024 & 960 & 32 & $\num{1.92e-07}$ & 49.7 & $\num{3.9e-06}$ & 1.30 \\
		1024 & 928 & 48 & $\num{5.47e-08}$ & 13.2 & $\num{1.0e-06}$ & 1.35 \\
		1024 & 896 & 64 & $\num{1.44e-08}$ & 2.8 & $\num{2.1e-07}$ & 1.31 \\
		1024 & 864 & 80 & $\num{5.87e-09}$ & 1.4 & $\num{1.1e-07}$ & 1.5 \\
		1024 & 832 & 96 & $\num{3.54e-10}$ & 0.0 & 0.0 & 1.58 \\
	\end{tabular}
\end{table}


\begin{table}[h!]
	\centering
	\caption{Error Metrics for FC-DenseNet Tile Size 1152}
	\label{tab:tile_size_1152}
	\begin{tabular}{r|r|r|r|r|r|r}
		TileSize & ZoR & Halo & RMSE    & ME & NME & RelativeRuntime \\ 
		\hline
		2304 & n/a & n/a & 0.0 & 0.0 & 0.0 & 1.0 \\
		1152 & 1152 & 0 & $\num{5.40e-03}$ & 821.5 & $\num{1.5e-04}$ & 1.15 \\
		1152 & 1088 & 32 & $\num{1.81e-03}$ & 376.1 & $\num{7.1e-05}$ & 1.42 \\
		1152 & 1024 & 64 & $\num{9.16e-04}$ & 168.0 & $\num{3.2e-05}$ & 1.54 \\
		1152 & 960 & 96 & $\num{3.36e-04}$ & 51.6 & $\num{9.7e-06}$ & 1.59 \\
		1152 & 896 & 128 & $\num{5.63e-05}$ & 5.3 & $\num{1.0e-06}$ & 1.67 \\
		1152 & 832 & 160 & $\num{4.97e-06}$ & 0.2 & $\num{4.7e-08}$ & 1.76 \\
		1152 & 768 & 192 & $\num{2.44e-07}$ & 0.0 & 0.0 & 2.32 \\
		1152 & 704 & 224 & $\num{1.92e-08}$ & 0.0 & 0.0 & 2.22 \\
		1152 & 640 & 256 & $\num{1.37e-08}$ & 0.0 & 0.0 & 2.33 \\
		1152 & 576 & 288 & $\num{1.44e-08}$ & 0.0 & 0.0 & 2.39 \\
		1152 & 512 & 320 & $\num{1.37e-08}$ & 0.0 & 0.0 & 2.52 \\
		1152 & 448 & 352 & $\num{1.45e-08}$ & 0.0 & 0.0 & 4.65 \\
		1152 & 384 & 384 & $\num{1.37e-08}$ & 0.0 & 0.0 & 5.89 \\
	\end{tabular}
\end{table}

\section{Appendix D: \texttt{SAME}  vs \texttt{VALID} Convolution Padding}

The original U-Net paper uses \texttt{VALID} type convolutions which shrink the spatial size of the feature maps by 2 pixels for each layer \cite{Dumoulin2018}\footnote{For an excellent review of convolutional arithmetic, including transposed convolutions (i.e., up-conv), see "A guide to convolutional arithmetic for deep learning by Dumoulin and Visin" \cite{Dumoulin2018}.}. 
Figure 2.1 from Dumoulin and Visin "A guide to convolutional arithmetic for deep learning" \cite{Dumoulin2018} shows an illustration showing why \texttt{VALID} causes the feature maps to shrink. The effect of \texttt{VALID} convolutions can also be seen in the first layer of the original U-Net where the input image size of $572 \times 572$ pixels shrinks to $570 \times 570$ \cite{Ronneberger2015a}. 
Switching to \texttt{SAME} type convolutions requires that within each convolutional layer zero padding is applied to each feature map to ensure the output has the same spatial size as the input. 
Figure 2.3 from Dumoulin and Visin \cite{Dumoulin2018} shows an illustration where a input $5 \times 5$ remains $5 \times 5$ after the convolution is applied. 
While \texttt{VALID} type convolutions avoid the negative effects of the zero padding within \texttt{SAME} type convolutions, which can affect the results as outlined by Huang et al. \cite{Huang2019a}, it is conceptually simpler to have input and output images of the same size. Additionally, our tiling scheme overcomes all negative effects that zero padding can introduce, justifying the choice of \texttt{SAME} type convolutions. 

The change to \texttt{SAME} type convolutions introduces an additional constraint on U-Net that needs to be mentioned. Given the skip connections between the encoder and decoder elements for matching feature maps, we need to ensure that the tensors being concatenated together are the same size. This can be restated as requiring the input size be divisible by the largest stride across the input image by any kernel. Another approach to ensuring consistent size between the encoder and decoder is to pad each up-conv layer to make its output larger and then crop to the target feature map size. We feel that is a less elegant solution than enforcing a tile size constraint and potentially padding the input image. 

The feature map at the bottleneck of U-Net is spatially $16 \times$ smaller than the input image. Therefore, the input to U-Net needs to be divisible by $16$.
As we go deeper into a network we trade spatial resolution for feature depth. Given a $512 \times 512$ pixel input image, the bottleneck shape will be $N \times 1024 \times 32 \times 32$ (assuming \texttt{NCHW}\footnote{NCHW Tensor dimension ordering: N (batch size), Channels, Height, Width} dimension ordering with unknown batch size). Thus the input image height divided by the bottleneck feature map height is $\frac{512}{32} = 16$. However, if the input image is $500 \times 500$ pixels, the bottleneck would be (in theory) $N \times 1024 \times 31.25 \times 31.25$. When there are not enough input pixels in a feature map to perform the $2 \times 2$ max pooling, the output feature map size is the floor of the input size divided by $2$. 
Thus, for an input image of $500 \times 500$ pixels the feature map heights after each max pooling layer in the encoder are: [$500, 250, 125, 62, 31$]. Now following the up-conv layers through the decoder, each of which doubles the spatial resolution, we end up with the following feature map heights: [$31, 62, 124, 248, 496$]. This results in a different feature map spatial size at the third level; encoder 125, decoder 124. If the input image size is a multiple of $16$ this mismatch cannot happen. 


%\end{document}

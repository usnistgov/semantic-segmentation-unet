% updated April 2002 by Antje Endemann
% Based on CVPR 07 and LNCS, with modifications by DAF, AZ and elle, 2008 and AA, 2010, and CC, 2011; TT, 2014; AAS, 2016; AAS, 2020

\documentclass[runningheads]{llncs}
\usepackage{graphicx}
\usepackage{comment}
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{color}
\usepackage{wrapfig}


\usepackage[binary-units]{siunitx}

% TODO setup camera ready document
\newif\ifcamera
\cameratrue
%\camerafalse


\ifcamera
% Do nothing
\else
\usepackage{ruler}
\usepackage[width=122mm,left=12mm,paperwidth=146mm,height=193mm,top=12mm,paperheight=217mm]{geometry}
\fi

\title{Error-Free Semantic Segmentation Inference of Images Larger than GPU Memory}

\pagestyle{headings}
\mainmatter
\def\ECCVSubNumber{5548}  % Insert your submission number here

\ifcamera
% CAMERA READY SUBMISSION
%******************
\titlerunning{Error-Free Out-of-Core FCN Inference}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here

%\author{First Author\inst{1}\orcidID{0000-1111-2222-3333} \and
%Second Author\inst{2,3}\orcidID{1111-2222-3333-4444} \and
%Third Author\inst{3}\orcidID{2222--3333-4444-5555}}

\author{Michael Majurski\inst{1} \and Peter Bajcsy\inst{1}}

\authorrunning{M. Majurski and P. Bajcsy}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{National Institute of Standards and Technology\\
	Information Technology Lab\\
	Gaithersburg, MD 20899, USA\\
	\email{\{michael.majurski,peter.bajcsy\}@nist.gov}}

%******************
\else
% INITIAL SUBMISSION 
%******************
\titlerunning{ECCV-20 submission ID \ECCVSubNumber} 
\authorrunning{ECCV-20 submission ID \ECCVSubNumber} 
\author{Anonymous ECCV submission}
\institute{Paper ID \ECCVSubNumber}
%******************
\fi



\begin{document}


\maketitle
 
%%%%%%%%% ABSTRACT
\begin{abstract}

We address the problem of performing error-free out-of-core semantic segmentation inference of arbitrarily large images using fully convolutional neural networks (FCN). FCN models have the property that once a model is trained it can be applied on arbitrarily sized images, although it is still constrained by the available GPU memory. This work is motivated by overcoming the GPU memory size constraint without numerically impacting the final result. 
Based on the past work, our approach is to select a tile size which will fit into GPU memory with a halo border of half the network receptive field. Next, stride across the image by the tile size without the halo. The input tile halos will overlap while the output tiles join exactly at the seams. Such an approach enables inference of whole slide microscopy images generated by a slide scanner, for example.

The novelty of this work is documenting the formulas for determining tile size and stride and then validating them on U-Net and FC-DenseNet architectures.
%The numerical accuracy is evaluated on a GPU with 24 GB of memory which can infer tiles as large as $\num{3584} \times \num{3584}$ pixels while the input images are $\num{20000} \times \num{20000}$ pixels. 

%This tiling scheme produces a segmentation output as if the whole result had been computed in a single forward pass. 
%The primary contribution of this work lies in demonstrating that one can achieve error-free inference using a tile-based approach.
% if the tiling parameters are chosen according to the mathematical analysis of the FCN model. 
In addition, we quantified the errors due to tiling configurations which do not satisfy the constraints, and explore using architecture effective receptive fields to estimate the tiling parameters. 


\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

The task of semantic segmentation, assigning a label to each image pixel, is often performed using deep learning based convolutional neural networks (CNNs) \cite{Badrinarayanan2015a,Ronneberger2015a}. A special type of CNNs which only uses convolutional layers is called "fully convolutional neural networks" (FCN) and it allows the alteration of the input image size. 
Both U-Net \cite{Ronneberger2015a} and the original FCN network \cite{Long2015} are examples of FCN type of CNNs. 
FCNs enable training the network on images much smaller than those of interest at inference time as long as the resolution is comparable. 
For example, one can train a U-Net model on $512 \times 512$ pixel tiles and then perform GPU-based inference on arbitrarily-sized images provided the GPU memory can accommodate the model coefficients, network activations, application code, and an input image tile. This decoupling of training and inference image sizes means the semantic segmentation models can be applied to images much larger than the memory available on current GPUs. 

The ability of FCN networks to inference arbitrarily large images differs from other types of CNNs where the training and inference image sizes must be identical. Usually this static image size requirement is not a problem since the input image size is expected to be static, or images can be resized within reason to fit the network. For example, if one trained a CNN on ImageNet \cite{Russakovsky2015} to classify pictures into two classes: \{Cat, Dog\}, the content of the image does not change drastically if the cat photo is resized to $224 \times 224$ pixels before inference provided the resolution is not altered considerably. Convolutional networks are not yet capable of strong generalization across scales \cite{Jaderberg2015,Lin2017a} so the inference time pixel resolution needs to approximately match the training time resolution or accuracy can suffer.

In contrast, there are applications where resizing the image is not acceptable due to loss of information. For example, in digital pathology, one cannot take a whole slide microscopy image generated by a slide scanner (upwards of $\num{10}$ Gigapixels) and fit it into GPU memory; nor can one reasonably resize the image as too much image detail would be lost. 

Our work is motivated by the need to design a methodology for arbitrarily large image inference on GPU memory constrained hardware in those applications where the loss of information due to image resizing is not acceptable. This method can be summarized as follows. Select a tile size which will fit into GPU memory with a halo border of half the network receptive field size. Next, stride across the image by the tile size without the halo. The input tile halos will overlap while the output tiles join exactly at the seams. 
The original U-Net paper \cite{Ronneberger2015a} briefly hinted at the feasibility of an inference scheme similar to the one we present in this paper but did not fully quantify and explain the inference scheme. 
The novelty of our work lies in presenting a methodology for error-free inference over images that are larger than available GPU memory for processed data. This work leverages FastImage \cite{Bardakoff2019}, a high-performance accessor library for processing gigapixel images in a tile-based manner.

\section{Related Work}
\label{related-work}

Out of core, tile-based processing is a common approach in the high performance computing (HPC) field where any local signal processing filter can be applied to carefully decomposed subregions of a larger problem \cite{Blattner2017}. The goal is often computation acceleration via task parallelization. These tile-based processes for the purpose of task parallelization also reduce the active working memory required at any given point in the computation.

It has been known since the initial introduction of FCN models that they can be applied via shift-and-stitch methods as if the FCN were a single filter \cite{Long2015,Sherrah2016}.
The original U-Net paper by Ronneberger et al. \cite{Ronneberger2015a} also hints at inference of arbitrary sized images in its Figure 2. However, none of the past papers mentioning shift-and-stitch discuss the methodology for performing out-of-core arbitrary sized image inference.

There are two common approaches for applying CNN models to large images: sliding window (overlapping tiles) and patch-based. Sliding window (i.e. overlapping tiles) has been used for object detection \cite{Sermanet2013,VanEtten2019} and for semantic segmentation \cite{Lin2019,Volpi2017a}. Patch-based inference also supports arbitrarily large images, but it is very inefficient \cite{Volpi2017a,Maggiori2016}.

Huang et al. and Iglovikov et al. \cite{Huang2019a,Iglovikov2017} both propose sliding window approaches.  Huang et al. \cite{Huang2019a} directly examines the problem of operating on images which cannot be inferred in a single forward pass. The authors focus on different methods for reducing but not eliminating the error in labeling that arises from different overlapping tile-based processing schemes. They examine label averaging and the impacts of different tile sizes on the resulting output error and conclude that using as large a tile as possible will minimize the error. Huang et al. \cite{Huang2019a} also examine the effects of zero-padding, documenting how much error it introduces . At no point do they produce error-free tile-based inference. Iglovikov et al. \cite{Iglovikov2017} remark upon error in the output logits at the tile edges during inference and suggest overlapping predictions or cropping the output to reduce that error. 

Patch based methods for dealing with large images predict a central patch given a larger local context. Volodymyr Mnih \cite{Mnih2013} predicts a $16 \times 16$ pixel patch from a $64 \times 64$ pixel area for road segmentation from aerial images. This approach will scale to arbitrarily large images and if a model architecture with a receptive field of less than $48$ pixels is used, it will have zero error from the tiling process. Saito et al. \cite{Saito2016} use a similar patch based formulation to Mihn, predicting a center patch given a large local context, however they add a model averaging component by predicting eight slightly offset versions of the same patch and then combining the predictions.

Our ability to perform error-free tile-based inference relies on the limited receptive field of convolutional layers. Luo et al. \cite{Luo2016} discuss the receptive fields of convolutional architectures, highlighting that for a given output pixel, there is limited context/information from the input that can influence that output\footnote{Article on "Computing Receptive Fields of Convolutional Neural Networks" \cite{araujo2019computing} \url{https://distill.pub/2019/computing-receptive-fields}}. Input data outside the receptive field cannot influence that output pixel \cite{Luo2016}. The receptive field of a model is highly dependent upon architecture and layer connectivity. We us the theoretical underpinning of receptive fields to identify a sufficient size of an image tile for error-free inference.  

To the best of our knowledge, no published method fully explores a methodology for error-free tile-based (out-of-core) inference of arbitrarily large images. While tile-based processing schemes have been outlined, the past publications do not provide a framework for achieving error-free tile-based inference results. Our approach does not handle layers with dynamic inference or variable receptive fields like stand alone self-attention \cite{Ramachandran2019b}, squeeze and excitation layers \cite{Hu2018}, deformable convolutions \cite{Dai2017}, or non-local layers \cite{Wang2018c}. 


\section{Methods}
\label{methods}

Inferencing arbitrarily large input images requires that we only inference a small enough tile to fit in GPU memory for any single forward pass and then operate tile-by-tile. To form a tile, the whole image being is broken down into non-overlapping regions. This is equivalent to striding across the image with a fixed stride. Then if one includes enough local context around each tile to cover the theoretical receptive field of the network, each pixel will have all of the local context it requires. Therefore, while the full image was broken into tiles for inference, each pixel individually had all of the information required to be predicted as if the whole image were passed through the network as one block of memory. Figure \ref{fig:nohalo} shows an example stem cell colony cropped from a larger $\num{20000} \times \num{20000}$ pixel microscope image (left), with the segmentation result produced by the model with a correct halo (center), and the segmentation result from tiling without the halo (right). 

\begin{figure}
	\centering
	\begin{minipage}{0.32\columnwidth}
		\includegraphics[width=\columnwidth]{figs/img.png}
	\end{minipage}
	\begin{minipage}{0.32\columnwidth}
		\includegraphics[width=\columnwidth]{figs/halo.png}
	\end{minipage}
	\begin{minipage}{0.32\columnwidth}
		\includegraphics[width=\columnwidth]{figs/no_halo.png}
	\end{minipage}
	\caption{(Left) Subregion of a grayscale stem cell colony image being segmented by U-Net. (Center) Segmentation result with proper halo border of 96 pixels. (Right) Segmentation result without a proper halo border, $halo = 0$.} 
	\label{fig:nohalo}
\end{figure}

%Given an FCN model architecture and a GPU with enough memory to inference at least a small (i.e. $512 \times 512$ pixel) image, we can construct a scheme for inferencing arbitrarily sized images. 
There are three important concepts required for this tile-based (out-of-core) processing scheme. 
\begin{enumerate}
	\item Zone of Responsibility (ZoR): a rectangular region (block, partition, zone, or area) of the output image currently being computed, a.k.a. the output tile size.
	\item Halo: minimum horizontal and vertical border around the ZoR indicating the local context that the FCN requires to accurately compute all pixels within the ZoR. This value is equal to half of the receptive field size of the model architecture being used. 
	\item Stride: the stride across the source image used to create tiles. The stride is fixed at the ZoR size.
\end{enumerate}

Each dimension of the square input tile is then defined as $inputTileSize = ZoR + 2 \times Halo$. 
Figure \ref{fig:zor} shows an example where a $832 \times 832$ pixel zone of responsibility is shown as a square with a $96$ pixel halo surrounding it. 
Since the local context provided by the pixels in the halo is required to correctly compute the output tile, the GPU input is $832 + 2 \times 96 = 1024$ pixels per spatial dimension. 

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/zor.png}
	\caption{Left: ZoR ($832 \times 832$ pixel square) with a $96$ pixel surrounding halo (shaded area) which combine to make the $1024 \times 1024$ pixel input tile required to infer the $832 \times 832$ pixel output tile (ZoR). Right: segmentation output tile showing the ZoR contribution to the final segmented image.}
	\label{fig:zor}
\end{figure}

In other words, the image is broken down into the non-overlapping ZoR (i.e. output tiles). For each ZoR, the local context defined by the halo (where that context is available) is included in the input tile to be passed through the network. For a specific output pixel, if an input pixel is more than half the receptive field away, it cannot influence that output pixel. Therefore including a halo of half the receptive field ensures that pixels on the edge of the ZoR (output tile) have all the required context.

After passing the input tile through the model, the ZoR within the prediction (without the halo) is copied to the output image being constructed in CPU memory. 
The halo provides the network with all the information it needs to make correct, deterministic predictions for the entirety of the output zone of responsibility. Hence the name, since each ZoR is responsible for a specific zone of the output image. 

This tile-based inferencing can be thought of as a series of forward passes, each computing a subregion (ZoR) of the feature maps that would be created while performing inference on the whole image in one pass. 
In summary, each tile's feature maps are created (forward pass), its ZoR output extracted, and then the GPU memory is recycled for the next tile. By building each ZoR result in a separate forward pass we can construct the network output within a fixed GPU memory footprint for arbitrarily large images. 

\subsection{U-Net Case Study}

We use U-Net \cite{Ronneberger2015a} as the case study of FCNs. Nonetheless, the presented methodology applies to any FCN, just the specific numerical values will be different. 

Note: for the purpose of brevity, we will use 'up-conv' (as the U-Net paper does) to refer to fractionally strided convolutions with a stride of $\frac{1}{2}$ which doubles the feature map spatial resolution \cite{Dumoulin2018}.

\subsubsection{Determining The Halo}

The halo must be half the receptive field. 
The general principle is to sum along the longest path through the network, the product of half the receptive field for each convolutional kernel and the stride that kernel has across the input image. The stride a specific convolution kernel has across the input image is a combination of that kernels stride with respect to its feature map and the downsampling factor between the input image size and the spatial size of that feature map. The downsampling factor is determined by the number of spatial altering layers between the input image and a specific layer. 

Let U-Net be described by an ordered sequence of convolutional layers $c={0, ..., N-1}$ with each layer being associated with a level $l_{c}$ and a square kernel $k_{c} \times k_{c}$. 
%A convolutional layer (1) convolves a kernel and its input feature maps to create a set of output feature maps, (2) applies an element-wise a non-linearity (ReLu \cite{lecun2015deep}), and (3) performs batch normalization \cite{ioffe2015batch} on its output feature maps.
For the network, $N$ defines the number of convolutional layers along the longest path from input to output. 

Let us define the level $l_{c}$ of an encoder-decoder network architecture as the number of max-pool\footnote{Convolutions with a stride of 2 can also be used to halve the spatial size of the feature maps but they will affect the receptive field.} layers minus the number of up-conv layers between the input image and the current convolutional layer $c$ along the longest path through the network. Levels start at 0; each max pool encountered along the longest path increases the level by 1 and each up-conv reduces the level by 1. 


\subsubsection{General Halo Calculation}

The required halo can be calculated according to the Equation \ref{eq:halo} for a general FCN architecture. 

\begin{equation}
Halo = \sum_{c=0}^{N-1} 2^{l_c} \lfloor \frac{k_c}{2} \rfloor
\label{eq:halo}
\end{equation}

The halo is a sum over every convolutional layer index $c$ from $0$ to $N-1$ encountered along the longest path from the input image to the output. 
Equation \ref{eq:halo} has two terms. The $2^{l_c}$ term is the number of pixels at the input image resolution that correspond to a single pixel within a feature map at level $l_c$. 
Therefore, at level $l_c=4$ each pixel in the feature map equates to $2^4 = 16$ pixels at the input image resolution. This $2^{l_c}$ term is multiplied by the second term $\lfloor \frac{k_c}{2} \rfloor$ which determines, for a given $c$, how many pixels of local context are required at that feature map resolution to perform the convolution.

\subsubsection{U-Net Configuration}

We have made two modifications to the published U-Net. 

\begin{enumerate}
	\item Normalization: Batch normalization \cite{ioffe2015batch} was added after the activation function of each convolutional layer as it is current good practice in the CNN modeling community. 
	\item Convolution Type: Convolutional type was changed to \texttt{SAME} from \texttt{VALID} as used in the original paper \cite{Ronneberger2015a}.
\end{enumerate} 

The use of batch normalization should prevent the out-of-core tiling scheme from achieving numerically identical results when performing inference of the whole image in a single pass because the batch statistics used for normalization will be different. However, as we will show later (Table \ref{tab:tile_size_512}), this appears to contribute very little to the observed error coming from using the tiling scheme.

The original U-Net paper uses \texttt{VALID} type convolutions which shrink the spatial size of the feature maps by 2 pixels for each layer \cite{Dumoulin2018}\footnote{For an excellent review of convolutional arithmetic, including transposed convolutions (i.e., up-conv), see "A guide to convolutional arithmetic for deep learning by Dumoulin and Visin" \cite{Dumoulin2018}.}. Switching to \texttt{SAME} type convolutions preserves feature map size. See Appendix D for additional explanation. 

There is one additional constraint on U-Net that needs to be mentioned. Given the skip connections between the encoder and decoder elements for matching feature maps, we need to ensure that the tensors being concatenated together are the same size. This can be restated as requiring the input size be divisible by the largest stride across the input image by any kernel. For U-Net this stride is $16$ (derivation in Appendix D). Another approach to ensuring consistent size between the encoder and decoder is to pad each up-conv layer to make its output larger and then crop to the target feature map size. That is a less elegant solution than enforcing a tile size constraint and potentially padding the input image.


\subsubsection{Halo Calculation for U-Net}

The published U-Net (Figure 1 from \cite{Ronneberger2015a}) has one level per horizontal stripe of layers. The input image enters on level $l_{c=0} = l_{c=1} = 0$. The first max-pool layer halves the spatial resolution of the network, changing the level. Convolution layers $c = \{2, 3\}$ after that first max-pool layer up to the next max-pool layer belong to level $l_{c=2}=l_{c=3} = 1$. This continues through level 4, where the bottleneck of the U-Net model occurs. In U-Net's Figure 1 \cite{Ronneberger2015a}, the bottleneck is the feature map at the bottom which occurs right before the first up-conv layer. After the bottleneck, the level number decreases with each subsequent up-conv layer, until level $l_{N-1} = 0$ right before the output image is generated. 

The halo computation in Equation \ref{eq:halo} can be simplified for U-Net as shown in Appendix A. Appendix B shows a numerical example for computing the halo size using Equation \ref{eq:halo}.

Following Equation \ref{eq:halo} for U-Net results in a minimum required halo of 92 pixels in order to provide the network with all of the local context it needs to predict the outputs correctly. This halo needs to be provided both before and after each spatial dimension and hence the input image to the network will need to be $2 \times 92 = 184$ pixels larger. This value is exactly the number of pixels the original U-Net paper has the output being shrunk by to avoid using \texttt{SAME} convolutions; a 572 pixel input shrunk by 184 results in the 388 pixel output \cite{Ronneberger2015a}. 
However, this runs afoul of our additional restriction on the U-Net input size, which requires images to be a multiple of 16. So rounding up to the nearest multiple of 16 results in a halo of 96 pixels. Since  one cannot just adjust the ZoR size to ensure $(ZoR + Halo) \% 16 = 0$ due to convolutional arithmetic, we must explore constraints on image partitioning. 

\subsection{Constraints on Image Partitioning}

Our tile-based processing methodology operates on the principle of constructing the intermediate feature map representations within U-Net in a tile-based fashion, such that they are numerically identical to the whole image being passed through the network in a single pass. Restated another way, the goal is to construct an input image partitioning scheme such that the zone of responsibility is building a spatial subregion of the feature maps that would exist if the whole image were passed through the network in a single pass. 

\subsubsection{Stride Selection}
To properly construct this feature map subregion one cannot stride across the input image in a different manner than would be used to inference the whole image. The smallest feature map in U-Net is spatially $16 \times$ smaller than the input image. Therefore, 16 pixels is the smallest offset one can have between two tile-based inference passes while having both collaboratively build subregions of a single feature map representation. Figure \ref{fig:offset} shows a simplified 1D example with a kernel of size 3 performing addition. When two applications of the same kernel are offset by less than the size of the kernel they can produce different results. 
%The output in the first row is \{$6, 15$\} whereas the output in the second row is \{$9, 18$\}. 
For U-Net, each $16 \times 16$ pixel block in the input image becomes a single pixel in the lowest spatial resolution feature map. A stride other than a multiple of 16 would result in subtly different feature maps because each feature map pixel was constructed from a different set of $16 \times 16$ input pixels. 

\begin{figure}
	\centering
		\includegraphics[width=0.8\linewidth]{figs/partitioning.png}
	\caption{A (left): Simplified 1D example of an addition kernel of size 3 being applied at an offset less than the kernel size, producing different results (compare top and bottom sums of 1, 2, and 3 or 2, 3, and 4).
	B (right): Simplified 1D example of reflection padding (reflected through dotted line) causing a different stride pattern across a set of pixels. The altered stride prevents the tile-based processing from collaboratively building subregions of a single feature map.}
	\label{fig:offset}
\end{figure}

This requirement means that we always need to start our tiling of the full image at the top left corner and stride across in a multiple of 16. However, this does not directly answer the question as to why we cannot have a non-multiple of 16 halo value. 

\subsubsection{Border Padding}
The limitation on the halo comes from the fact that if we have arbitrary halo values, we will need to use different padding schemes between the full image inference and the tile-based inference to handle the image edge effects. Figure \ref{fig:offset} shows for a 1D case how reflection padding can (1) alter the stride across the full image which needs to be maintained as a multiple of 16 to collaboratively build subregions of a single feature map and (2) change the reflection padding required to have an input image whose spatial dimensions are a multiple of 16. Reflection padding is preferred since it preserves image statistics locally.

\subsubsection{ZoR and Halo Constraints}

Both problems, (1) collaboratively building feature maps and (2) different full image edge reflection padding requirements disappear if both the zone of responsibility and the halo are multiples of 16. Thus, we constrain the final values of ZoR and halo to be the closest higher multiple of the ratio $F$ between the image size $I$ and minimum feature map size (Equation \ref{eq:constraints}) where $F=16$ for the published U-Net, 
\begin{equation}
\begin{aligned}
F = \frac{ \min\{H_{I}, W_{I} \} }{ \min_{\forall l_{c}} \{ H_{l_{c}}, W_{l_{c}} \} } \\
Halo^{*} = F \lceil \frac{Halo}{F} \rceil \\
ZoR = F \lceil \frac{ZoR}{F} \rceil 
\end{aligned}
\label{eq:constraints}
\end{equation}
where $H_{I}$ and $W_{I}$ are the input image height and width dimensions, $Halo^{*}$ is the adjusted halo value to accommodate stride constraints, and $H_{l_{c}}$ and $W_{l_{c}}$ are the feature map height and width dimensions. 

\section{Experimental Results}
\label{experimental-results}

\subsection{Dataset}
\label{dataset}

We used a publicly accessible dataset acquired in phase contrast imaging modality and published in \cite{Bhadriraju2016}. 
The dataset consists of three collections, each with around 161 time-lapse images at roughly $\num{20000} \times \num{20000}$  pixels per stitched image frame with 2 bytes per pixels. 


\subsection{Error-Free Tile-Based Inference Scheme}

Whether inferencing the whole image in a single forward pass or using tile-based processing, the input image size needs to be a multiple of 16 as previously discussed. Reflection padding is applied to the input image to enforce this size constraint before the image is decomposed into tiles. 
%When a whole image is inferred in a single forward pass the image size needs to be a multiple of 16. To achieve this reflection padding is used when required on the bottom and right of the image to extend its size. The same size requirement exists when performing tile-based processing. 

Let us assume that we know how big an image we can fit into GPU memory, for example $1024 \times 1024$ pixels. Additionally, given that we are using U-Net we know that the required halo is 96 pixels. Then our zone of responsibility is $ZoR = 1024 - 2 \times Halo = 832$ pixels per spatial dimension. 
%Refer back to Figure \ref{fig:zor} for a visual. 
Despite inferencing $1024 \times 1024$ pixel tiles on the GPU per forward pass, the stride across the input image is 832 pixels because we need non-overlapping ZoR. 
%Figure \ref{fig:all-zor} shows a full image with 6 non-overlapping ZoR in alternating colors (red and blue) with each halo as a shaded region surrounding each ZoR. 
The edges of the full image do not require halo context to ensure identical results when compared with a single inference pass. Intuitively, the true context is unknown since it's outside the existing image.

%Image tiling starts at $[x_{st}, y_{st}, x_{end}, y_{end}] = [0, 0, 832, 832]$ and proceeds across the image with stride 832. 
In the last row and column of tiles, there might not be enough pixels to fill out a full $1024 \times 1024$ tile. However, because U-Net can alter its spatial size on demand, as long as the tile is a multiple of 16 a narrower (last column) or shorter (last row) tile can be used. 


\subsection{Errors due to Small Halo}

To experimentally confirm that our out-of-core image inference methodology does not impact the results we determined the largest image we could process on our GPU, performed the forward pass, and saved the resulting softmax output values as ground truth data. We then process the same image using our tiling scheme with varying halo values. We show that there are numerical differences (greater than floating point error) when using halo values less 96. 

We trained our U-Net model to perform binary (foreground/background) segmentation of the phase contrast microscopy images. The largest image we could inference on our GPU with $\SI{24}{\giga\byte}$ of memory is $3584 \times 3584$ pixels. We created 20 reference inference results by cropping out $K = 20$ random $3584 \times 3584$ subregions of the dataset. 

We performed tile-based out-of-core inference for each of the 20 reference images using a tile size of 512 pixels, meeting the multiple of 16 constraint. Halo values from 0 to 96 pixels were evaluated in 16 pixel increments. 

The tiling codebase seamlessly constructs the softmax output in CPU memory as if the whole image had been inferred in a single forward pass. So our evaluation methodology consists of looking for differences in the output softmax produced by the reference forward pass ($R$) as well as the tile-based forward pass ($T$). We used the following two metrics for evaluation: Root Mean Squared Error (RMSE, Equation \ref{eq:rmse}) of the softmax and Misclassification Error (ME, Equation \ref{eq:me}) of the resulting binary segmentation masks. %ME is the number of misclassified pixels, which provides a very intuitive understanding of the error since it directly measures how many output pixels were incorrect. 
We also include Normalized Misclassification Error (NME, Equation \ref{eq:nme}), the ME normalized by the number of pixels; and Relative Runtime, where compute time to perform inference is shown relative by the runtime without the tiling scheme. This runtime highlights that there is a tradeoff to be made between the error introduced due to the out-of-core GPU inference and the compute overhead required to do so. The NME metric can be multiplied by $100$ to compute the percent of pixels with errors due to the tiling scheme.
All metrics are averaged across the $K = 20$ reference images.

\begin{equation}
RMSE = \frac{1}{K} \sum_{i=1}^{K} \sqrt{ \frac{\sum_{i = 1}^{m} \sum_{j = 1}^{n} (R_{ij} - T_{ij})^2}{mn}}
\label{eq:rmse}
\end{equation}

\begin{equation}
ME = \frac{1}{K} \sum_{i=1}^{K} \left( \sum_{i = 1}^{m} \sum_{j = 1}^{n} [ R_{ij} \neq T_{ij} ] \right) 
% https://en.wikipedia.org/wiki/Iverson_bracket
\label{eq:me}
\end{equation}

\begin{equation}
NME = \frac{1}{K} \sum_{i=1}^{K} \left( \frac{\sum_{i = 1}^{m} \sum_{j = 1}^{n} [ R_{ij} \neq T_{ij} ]}{nm} \right) 
% https://en.wikipedia.org/wiki/Iverson_bracket
\label{eq:nme}
\end{equation}

The total inference error is a composite of model error (what is directly minimized by gradient descent during training) and tiling error. We demonstrate a zero error contribution from tiling in the case of trained and untrained U-Net models (or minimum and maximum inference errors due to a model).
  
For the trained model, the error metrics are shown in Table \ref{tab:tile_size_512} with 512 pixel tiles. Once the required 96 pixel halo is met the RMSE falls into the range of floating point error and ME goes to zero. Beyond the minimum required halo, all error metrics remain equivalent to the minimum halo. The first row shows the data for the whole image being inferred without the tiling scheme. 
The ME metric is especially informative, because when it is zero, the output segmentation results are identical regardless of whether the whole image was inferred in a single pass or it was decomposed into tiles. Table \ref{tab:tile_size_512} highlights the engineering tradeoff that must be made, obtaining zero inference error requires $2.69 \times$ the wall clock runtime. The normalized error (NME) with naive tailing is $\num{6.1e-4}$ or $\num{0.06} \%$. Depending on your application, this level of error might be acceptable despite the potential for edge effects between the non-overlapping tiles. One consideration, larger tile sizes are more compute efficient because the ratio of the ZoR area to the tile area increases. 

\begin{table}
	\centering
\caption{Error Metrics for Tile Size 512}
\label{tab:tile_size_512}
\begin{tabular}{r|r|r|r|r|r|r}
	TileSize & ZoR & Halo & RMSE    & ME & NME & RelativeRuntime \\ 
	\hline
3584 & n/a & n/a & 0.0 & 0.0 & 0.0 & 1.0 \\
512 & 512 & 0 & $\num{1.11e-02}$ & 7773.4 & $\num{6.1e-04}$ & 1.08 \\
512 & 480 & 16 & $\num{6.35e-03}$ & 5455.4 & $\num{4.2e-04}$ & 1.31 \\
512 & 448 & 32 & $\num{3.29e-03}$ & 2372.2 & $\num{1.8e-04}$ & 1.36 \\
512 & 416 & 48 & $\num{1.95e-03}$ & 1193.7 & $\num{9.3e-05}$ & 1.61 \\
512 & 384 & 64 & $\num{7.79e-04}$ & 434.1 & $\num{3.4e-05}$ & 1.85 \\
512 & 352 & 80 & $\num{1.50e-04}$ & 71.6 & $\num{5.6e-06}$ & 2.21 \\
512 & 320 & 96 & $\num{4.17e-10}$ & 0.0 & 0.0 & 2.58 \\
\end{tabular}
\end{table}

For the untrained model, results are shown in Table \ref{tab:tile_size_1024} in Appendix C with 1024 pixel tiles. The results were generated using an untrained 4 class U-Net model, whose weights were left randomly initialized. Additionally, the image data for that result was normally distributed random noise with $\mu = 0, \sigma = 1$. The error coming from tile-based processing is zero once the required halo is met.


\subsection{Errors due to Violation of Partitioning Constraints}

To demonstrate how the inference results differ as a function of how the network strides across the input image we have constructed 32 overlapping, $2048 \times 2048$ pixel subregions of an image; each offset from the previous subregion start by 1 pixel. So the first subregion is $[x_{st}, y_{st}, x_{end}, y_{end}] = [0, 0, 2048, 2048]$, while the second subregion is $[1, 0, 2049, 2048]$, and so on. In order to compare the inference results without any edge effects confounding the results, we only compute RMSE (Equation \ref{eq:rmse}) of the softmax output within the area in common between all 32 images, inset by 96 pixels; $[128, 96, 1920, 1952]$. The results are shown in Figure \ref{fig:stride-impact} where identical softmax outputs only happen when the offset is a multiple of 16.

\begin{wrapfigure}{r}{0.44\textwidth}
	\includegraphics[width=0.44\textwidth]{figs/stride_impact.png}
		\caption{Impact of the stride offset on the RMSE of the U-Net softmax output.}
		\label{fig:stride-impact}
\end{wrapfigure} 
	
%\begin{figure}
%		\centering
%		\includegraphics[width=0.6\linewidth]{figs/stride_impact.png}
%	\caption{Impact of the stride offset on the RMSE of the U-Net softmax output.}
%	\label{fig:stride-impact}
%\end{figure} 

\subsection{Application to a Fully Convolutional DenseNet}

Up to this point we have shown that our ZoR and halo tiling scheme produces error-free out-of-core semantic segmentation inference for arbitrarily large images when using the published U-Net architecture \cite{Ronneberger2015a}. 
This section demonstrates the tiling scheme on a Fully Convolutional DenseNet configured for Semantic Segmentation \cite{Jegou2017}. DenseNets \cite{Huang2017} replace stacked convolutional layers with densely connected blocks where each convolutional layer is connected to all previous convolutional layers in that block. Jegou et al. \cite{Jegou2017} extended this original DenseNet idea to create a fully convolutional DenseNet based semantic segmentation architecture. 

While the architecture of DenseNets significantly differs from U-Net, the model is still fully convolutional and thus our tiling scheme is applicable. Following Equation \ref{eq:halo} for a FC-DenseNet-56 \cite{Jegou2017} model produces a required halo value of 377. This is significantly higher than U-Net due to the architecture depth. FC-DenseNet-56 also has a ratio between the input image size and the smallest feature map of $F = 32$. Therefore the inference image sizes need to be a multiple of 32, not 16 like the original U-Net. Thus, the computed 377 pixel halo is adjusted up to 384. 

The error metrics for FC-DenseNet-56 as a function of halo are showing in Table \ref{tab:tile_size_1152} in Appendix C. This numerical analysis relies on versions of the same 20 test images from the U-Net analysis, but cropped to $\num{2304} \times \num{2304}$, which was the largest image we were able to inference using FC-DenseNet-56 on our $\SI{24}{\giga\byte}$ GPU in a single forward pass. 


\subsection{Halo Approximation via Effective Receptive Field}

The halo value required to achieve error free inference increases with the depth of the network. For example, see Table \ref{tab:common_radii} which shows the theoretical halo values (computed using Equation \ref{eq:halo}) for a few common semantic segmentation architectures. The deeper networks, like FC-DenseNet-103 require very large halo values to guarantee error free tile based inference.

\begin{table}
	\centering
	\caption{Theoretical Radii for Common Segmentation Architectures}
	\label{tab:common_radii}
	\begin{tabular}{r|r|r}
		Architecture & Halo &  \\ 
		\hline
		U-Net \cite{Ronneberger2015a} & 96 \\
		SegNet \cite{Badrinarayanan2015a} & 192 \\
		FCN-VGG16 \cite{Long2015} & 202 \\
		FC-DenseNet-56 \cite{Jegou2017} & 384 \\
		FC-DenseNet-67 \cite{Jegou2017} & 480 \\
		FC-DenseNet-103 \cite{Jegou2017} & 1120 \\
	\end{tabular}
\end{table}

Using the empirical effective receptive field estimation method outlined by Luo et al. \cite{Luo2016} which consists of setting the loss to 1 in the center of the image and then back propagating to the input image, we can automatically estimate the required halo. For our trained U-Net \cite{Ronneberger2015a} this method produces an estimated halo of 96 pixels, which is exactly the theoretical halo. This matches the data in Tables \ref{tab:tile_size_512} and \ref{tab:tile_size_1024} where the ME metric did not fall to zero until the theoretical halo was reached. On the other hand, according to Table \ref{tab:tile_size_1152} for FC-DenseNet-56, there is no benefit to using a halo larger than 192 despite the theoretical required halo (receptive field) being much larger. This is supported by the effective receptive field estimated halo of 160 pixels; just below the empirically discovered minimum halo of 192. Thus, using the effective receptive field for estimating the required halo is not foolproof but it provides a good proxy for automatically reducing the tiliing error. 


\section{Conclusions}
\label{conclusion}

This paper outlined a methodology for performing error-free segmentation inference of arbitrarily large images. 
We documented the formulas for determining the tile-based inference scheme parameters. We then demonstrated that the inference results are identical regardless of whether or not tiling was used. These inference scheme parameters were related back to the theoretical and effective receptive fields of deep convolutional networks as previously studied in literature \cite{Luo2016}. The empirical effective receptive field estimation methods of Luo et al. \cite{Luo2016} were used to provide a rough estimate of the inference tiling scheme parameters without requiring any knowledge of the architecture.
While we used U-Net and FC-DenseNets as example FCN models, these principles apply to any FCN model while being robust across different choices of tile size. 

In this work we did not consider any FCN networks with dilated convolutions, which are known to increase the receptive field side of the network. We will include this extension in future work as well as tradeoff evaluations of the relationships among relative runtime, GPU memory size and maximum tile size.

%   While the dilation factor should be a multiplicative influence on the $2^{l_c}$ term in Equation \ref{eq:halo}; this would need to be empirically validated in future work. 

% TODO include future work discussion about building the finaly layer feature map for things like classification or regression networks (resnet50) using a tiling scheme and then performing the final layer calculation on CPU in order to inference arbitrarily large images for classification and regresion. Mention that those problems are less common than having to segment an entire whole slide image.


\section{Test Data and Source Code}
The test data and the Tensorflow v2.x source code are available from public URLs\ifcamera\footnote{
\url{https://isg.nist.gov/deepzoomweb/data/stemcellpluripotency} 
\url{https://github.com/usnistgov/semantic-segmentation-unet/tree/ooc-inference}.}\fi.
While the available codebase in theory supports arbitrarily large images, we made the choice at implementation time to load the whole image into memory before processing it through the network. In practice this means the codebase is limited to inferencing images which fit into CPU memory. However, using a file format which supports reading sub-sections of the whole image would support inference of disk-backed images which do not fit into CPU memory. 

\ifcamera
\section{Disclaimer}

Commercial products are identified in this document in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by NIST, nor is it intended to imply that the products identified are necessarily the best available for the purpose. Analysis performed [in part] on the NIST Enki HPC cluster. Contribution of U.S. government not subject to copyright.
\fi

\clearpage
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
{\fontsize{9.0pt}{10.0pt} 
\selectfont
\bibliography{ooc-inference}
\bibliographystyle{splncs04}

\clearpage
\input{ooc-inference-appendix}
}




\end{document}

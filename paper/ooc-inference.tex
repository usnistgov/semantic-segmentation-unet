%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     LaTeX source code to approximate a Journal of Research of NIST article
%	  Instructions for authors: tinyurl.com/jresnist
% 	Developed by K. Miller, kmm5@nist.gov 
%	Last updated: 27-Dec-2019
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
 
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     Metadata -- to be completed by Publisher
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\newcommand{\volume}{XXX}
\newcommand{\volumeyear}{XXXX}
\newcommand{\articlenum}{XXXXXX}
\newcommand{\journaltitle}{Journal of Research of National Institute of Standards and Technology}
\newcommand{\DOI}{\url{https://doi.org/10.6028/jres.XXX.XXX}}
\newcommand{\accepted}{Month Date, Year}
\newcommand{\published}{Month Date, Year}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     Manuscript information -- to be completed by Author
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\newcommand{\papertitle}{Exact Tile-Based Segmentation Inference of Images Larger than GPU Memory}
\newcommand{\authorlist}{Michael Majurski & Peter Bajcsy}
%  \newcommand{\dataDOI}{\url{https://doi.org/...}}

% Original ERB Reviewers:
% Control number G2019-1404
% In Division: Sarala Padi 
% Outside Division: Petre Manescu P.Manescu@cs.ucl.ac.uk
%
% NIST JR Reviewers Derek Juba, Adele Peskin

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     Set page format
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10pt, indentfirst]{article}

\usepackage{amsmath}
\usepackage{amsfonts}   % if you want the fonts
\usepackage{amssymb}    % if you want extra symbols
\usepackage{graphicx}   % need for figures
\usepackage{xcolor}
\usepackage{bm}
\usepackage{secdot}
\usepackage{mathptmx}
\usepackage{float}
\usepackage{titlesec}

\usepackage[binary-units]{siunitx}
\usepackage{comment}
\usepackage{wrapfig}

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}


\titleformat{\section}{\large\bfseries}{\thesection.}{1em}{}
\titleformat*{\subsection}{\normalsize\bfseries}
\usepackage{fancyhdr}
\fancypagestyle{title}{%
  
\fancyhf{}%
\renewcommand{\headrulewidth}{.5pt}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\baselinestretch}{1.1}

\fancyhead[c]{\small Volume \volume, Article No. \articlenum ~(\volumeyear) \DOI \\ 
\large \journaltitle  \\}
\fancyfoot[C]{\thepage}
\fancyfoot[R]
{\small \textbf
{How to cite this article:}
\\
Majurski M (\volumeyear) \papertitle.\\
\textit{J Res Natl Inst Stan} \volume:\articlenum. \DOI.}
}%




\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage[hang,flushmargin]{footmisc} 
\usepackage[numbers,sort&compress]{natbib}
\usepackage[numbib,nottoc]{tocbibind}
\setlength{\bibsep}{0.0pt}

\usepackage[hidelinks]{hyperref}
\hypersetup{
	colorlinks = true,
urlcolor ={blue},
citecolor = {.},
linkcolor = {.},
anchorcolor = {.},
filecolor = {.},
menucolor = {.},
runcolor = {.}
}
\urlstyle{same}


\usepackage{indentfirst}
\usepackage{fancyhdr}
%\usepackage{showframe}
\usepackage{caption} 
\captionsetup{font=small,labelfont=bf,figurename=Fig.,labelsep=period,justification=raggedright}

\setlength{\topmargin}{-0.5in}
\setlength{\headheight}{39pt}
\setlength{\oddsidemargin}{0.25in}
\setlength{\evensidemargin}{0.25in}
\setlength{\textwidth}{6.0in}
\setlength{\textheight}{8.5in}

\newcommand{\vv}[1]{{\hat{#1}}}
\renewcommand{\baselinestretch}{1.1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     First page content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\setcounter{page}{1}


\title{\Huge{\textit{\papertitle}}
}
\date{\vspace{-7ex}}

\vspace*{-70pt}{\let\newpage\relax\maketitle}
\maketitle
\thispagestyle{title}
\begin{flushleft} \normalsize \textbf{Michael Majurski$^1$, and Peter Bajcsy$^1$} \\
	\vspace{9pt} 
	\normalsize $^1$National Institute of Standards and Technology, \\  
	Gaithersburg, MD 20899 \\
	
	\vspace{15pt} 
	\footnotesize \url{michael.majurski@nist.gov}\\
	\footnotesize \url{peter.bajcsy@nist.gov}\\
	\vspace{20pt} 
	
We address the problem of performing exact (tiling-error free) out-of-core semantic segmentation inference of arbitrarily large images using fully convolutional neural networks (FCN). 
FCN models have the property that once a model is trained it can be applied on arbitrarily sized images, although it is still constrained by the available GPU memory. 
This work is motivated by overcoming the GPU memory size constraint without numerically impacting the final result. 
Our approach is to select a tile size which will fit into GPU memory with a halo border of half the network receptive field. 
Next, stride across the image by the tile size without the halo. The input tile halos will overlap while the output tiles join exactly at the seams. 
Such an approach enables performing inference of whole slide microscopy images generated by a slide scanner, for example.
The novelty of this work is documenting the formulas for determining tile size and stride and then validating them on U-Net and FC-DenseNet architectures.
In addition, we quantified the errors due to tiling configurations which do not satisfy the constraints, and explore using architecture effective receptive fields to estimate the tiling parameters. 
\\
	
	
	
	\vspace{8pt} 
	\textbf{Key words:} artificial intelligence; convolutional neural networks; effective receptive field; out-of-core processing; semantic segmentation. \\ %% Keywords must be alphabetized, separated by semicolon, and end in a period. 
	\vspace{8pt} 
	\textbf{Accepted:} \accepted \\
	\vspace{8pt} 
	\textbf{Published:} \published \\ %% JRes NIST will add this in before publication
	\vspace{8pt} 
	\DOI
\end{flushleft}
\vspace{-15pt}
\LARGE \hrulefill

\setlength{\abovedisplayskip}{19pt}
\setlength{\belowdisplayskip}{19pt}
\setlength{\abovedisplayshortskip}{9pt}
\setlength{\belowdisplayshortskip}{19pt}

\raggedright
\setlength\parindent{16pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%     Rest of manuscript content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\renewcommand{\headrulewidth}{.5pt}
  \renewcommand{\footrulewidth}{0pt}
  \fancyhead{}

  \fancyhead[c]{\small Volume \volume, Article No. \articlenum ~(\volumeyear) \DOI \\ 
	\large \journaltitle \\}

\fancyfoot{} 
\fancyfoot[c]{\thepage}
\fancyfoot[r]{\DOI}



%%%%%%%%% BODY TEXT
\section{Introduction}


\normalsize

The task of semantic segmentation, assigning a label to each image pixel, is often performed using deep learning based convolutional neural networks (CNNs) \citep{Badrinarayanan2015a,Ronneberger2015a}.
A subtype type of CNN which only uses convolutional layers is called a "fully convolutional neural network" (FCN), which can be used with input of arbitrary size.
Both U-Net \citep{Ronneberger2015a} and the original FCN network \citep{Long2015} are examples of FCN type CNNs.
FCNs enable training the network on images much smaller than those of interest at inference time as long as the resolution is comparable.
For example, one can train a U-Net model on $512 \times 512$ pixel tiles and then perform GPU-based inference on arbitrarily-sized images provided the GPU memory can accommodate the model coefficients, network activations, application code, and an input image tile.
This decoupling of training and inference image sizes means the semantic segmentation models can be applied to images much larger than the memory available on current GPUs.

Images larger than GPU memory can also be used to train the model if you first break them into tiles.
In fact, that is fairly common practice when working with large format images.
Interestingly, the same edge effect which cause tiling errors during inference also affects the training process.
Therefore the tiling methodology presented here could also be used to generate tiles for network training.

The ability of FCN networks to perform inference on arbitrarily large images differs from other types of CNNs where the training and inference image sizes must be identical.
Usually this static image size requirement is not a problem since the input image size is expected to be static, or images can be resized within reason to fit the network.
For example, if one trained a CNN on ImageNet \citep{Russakovsky2015} to classify pictures into two classes: \{Cat, Dog\}, the content of the image does not change drastically if the cat photo is resized to $224 \times 224$ pixels before inference provided the resolution is not altered considerably.
Convolutional networks are not yet capable of strong generalization across scales \citep{Jaderberg2015,Lin2017a} so the inference time pixel resolution needs to approximately match the training time resolution or accuracy can suffer.

In contrast, there are applications where resizing the image is not acceptable due to loss of information.
For example, in digital pathology, one cannot take a whole slide microscopy image generated by a slide scanner (upwards of $\num{10}$ Gigapixels) and fit it into GPU memory; nor can one reasonably resize the image as too much image detail would be lost. 

Our work is motivated by the need to design a methodology for performing inference on arbitrarily large images on GPU memory constrained hardware in those applications where the loss of information due to image resizing is not acceptable.
This method can be summarized as follows.
The image is broken down into non-overlapping tiles.
Each tile has its local context defined by the halo border (ghost region) included only when performing inference.
The tile size should be selected so that when the halo border is included the whole image will fit into GPU memory.
The network receptive field \citep{araujo2019computing} is the set of all input pixels which can influence a given output pixel.
We use the receptive field to define the halo border as half the network receptive field to ensure that pixels on the edge of the non-overlapping tiles have all of the required local context as if computed in a single forward pass.

There are three important concepts required for this tile-based (out-of-core) processing scheme.
\begin{enumerate}
	\item Zone of Responsibility (ZoR): a rectangular region of the output image currently being computed, a.k.a. the output tile size.
	\item Halo: minimum horizontal and vertical border around the ZoR indicating the local context that the FCN requires to accurately compute all pixels within the ZoR.
	This value is equal to half of the receptive field size of the model architecture being used.
	This is derived from the definition of the convolution network: each output pixel in the ZoR is dependent upon other pixels in either the ZoR or in the halo.
	\item Stride: the stride across the source image used to create tiles.
	The stride is fixed at the ZoR size.
\end{enumerate}

The original U-Net paper \citep{Ronneberger2015a} briefly hinted at the feasibility of an inference scheme similar to the one we present in this paper but did not fully quantify and explain the inference mechanism.
The novelty of our work lies in presenting a methodology for tiling-error-free inference over images that are larger than available GPU memory for processed data.

\section{Related Work}
\label{related-work}

Out-of-core, tile-based processing is a common approach in the high performance computing (HPC) field where any local signal processing filter can be applied to carefully decomposed subregions of a larger problem \citep{Blattner2017}.
The goal is often computation acceleration via task or data parallelization.
These tile-based processes for the purpose of task parallelization also reduce the active working memory required at any given point in the computation.

It has been known since the initial introduction of FCN models that they can be applied via shift-and-stitch methods as if the FCN were a single filter \citep{Long2015,Sherrah2016}.
The original U-Net paper \citep{Ronneberger2015a} also hints at inference of arbitrary sized images in its Figure 2.
However, none of the past papers mentioning shift-and-stitch discuss the methodology for performing out-of-core arbitrary sized image inference.

There are two common approaches for applying CNN models to large images: sliding window (overlapping tiles) and patch-based.
Sliding windows (i.e. overlapping tiles) have been used for object detection \citep{Sermanet2013,VanEtten2019} and for semantic segmentation \citep{Lin2019,Volpi2017a}.
Patch-based inference also supports arbitrarily large images, but it can be very inefficient \citep{Volpi2017a,Maggiori2016}.

Huang et al. and Iglovikov et al. \citep{Huang2019a,Iglovikov2017} both propose sliding window approaches.
Huang et al. \citep{Huang2019a} directly examines the problem of operating on images for which inference cannot be performed in a single forward pass.
The authors focus on different methods for reducing but not eliminating the error in labeling that arises from different overlapping tile-based processing schemes.
They examine label averaging and the impacts of different tile sizes on the resulting output error and conclude that using as large a tile as possible will minimize the error.
Huang et al. \citep{Huang2019a} also examine the effects of zero-padding, documenting how much error it introduces.
At no point do they produce tiling-error free inference.
Iglovikov et al. \citep{Iglovikov2017} remark upon error in the output logits at the tile edges during inference and suggest overlapping predictions or cropping the output to reduce that error.

Patch based methods for dealing with large images predict a central patch given a larger local context.
Mnih \citep{Mnih2013} predicts a $16 \times 16$ pixel patch from a $64 \times 64$ pixel area for road segmentation from aerial images.
This approach will scale to arbitrarily large images and if a model architecture with a receptive field of less than $48$ pixels is used, it will have zero error from the tiling process.
Saito et al. \citep{Saito2016} use a similar patch based formulation to Mnih, predicting a center patch given a large local context, however they add a model averaging component by predicting eight slightly offset versions of the same patch and then combining the predictions.

Our ability to perform error-free tile-based inference relies on the limited receptive field of convolutional layers.
Luo et al. \citep{Luo2016} discuss the receptive fields of convolutional architectures, highlighting that for a given output pixel, there is limited context/information from the input that can influence that output \citep{araujo2019computing}.
Input data outside the receptive field cannot influence that output pixel \citep{Luo2016}.
The receptive field of a model is highly dependent upon architecture and layer connectivity.
We use the theoretical underpinning of receptive fields to identify a sufficient size of an image tile for performing inference without tiling-based errors.

To the best of our knowledge, no published method fully explores a methodology for performing error-free tile-based (out-of-core) inference of arbitrarily large images.
While tile-based processing schemes have been outlined, the past publications do not provide a framework for achieving error-free tile-based inference results.
Our approach does not handle layers with dynamic inference or variable receptive fields like stand alone self-attention \citep{Ramachandran2019b}, squeeze and excitation layers \citep{Hu2018}, deformable convolutions \citep{Dai2017}, or non-local layers \citep{Wang2018c}.


\section{Methods}
\label{methods}

Performing inference on arbitrarily large input images requires that we operate based on image tiles and only on a small enough tile to fit in GPU memory for any single forward pass.
To form a tile, the whole image is broken down into non-overlapping regions.
This is equivalent to striding across the image with a fixed stride.
Then if we include enough local context around each tile to cover the theoretical receptive field of the network, each pixel will have all of the local context it requires.
Therefore, while the full image is broken into tiles to performing inference, each pixel individually has all of the information required to be predicted as if the whole image were passed through the network as one block of memory.
This work leverages FastImage \citep{Bardakoff2019}, a high-performance accessor library for processing gigapixel images in a tile-based manner.
Figure \ref{fig:nohalo} shows a cropped region from an example $\num{20000} \times \num{20000}$ pixel stem cell microscope image (left), with the segmentation result produced by the model applied to the full image with a correct halo (center), and the segmentation result from tiling without the halo (right).

\begin{figure}
	\centering
	\begin{minipage}{0.32\columnwidth}
		\includegraphics[width=\columnwidth]{figs/img.png}
	\end{minipage}
	\begin{minipage}{0.32\columnwidth}
		\includegraphics[width=\columnwidth]{figs/halo.png}
	\end{minipage}
	\begin{minipage}{0.32\columnwidth}
		\includegraphics[width=\columnwidth]{figs/no_halo.png}
	\end{minipage}
	\caption{(Left) $250 \times 250$ pixel subregion of a grayscale stem cell colony image being segmented by U-Net.
		(Center) Segmentation result with proper halo border of 96 pixels.
		(Right) Segmentation result with artifacts due to tiling (halo border is equal to $0$).} 
	\label{fig:nohalo}
\end{figure}

Each dimension of the square input tile is then defined as $inputTileSize = ZoR + 2 \times Halo$.
Figure \ref{fig:zor} shows an example where a $832 \times 832$ pixel zone of responsibility is shown as a square with a $96$ pixel halo surrounding it.
Since the local context provided by the pixels in the halo is required to correctly compute the output tile, the GPU input is $832 + 2 \times 96 = 1024$ pixels per spatial dimension.

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figs/zor.png}
	\caption{Left: ZoR ($832 \times 832$ pixel square) with a $96$ pixel surrounding halo (shaded area) which combine to make the $1024 \times 1024$ pixel input tile required to infer the $832 \times 832$ pixel output tile (ZoR).
		Right: segmentation output tile showing the ZoR contribution to the final segmented image.}
	\label{fig:zor}
\end{figure}

In other words, the image is broken down into the non-overlapping ZoR (i.e. output tiles).
For each ZoR, the local context defined by the halo (where that context is available) is included in the input tile to be passed through the network.
For a specific output pixel, if an input pixel is more than half the receptive field away, it cannot influence that output pixel.
Therefore including a halo of half the receptive field ensures that pixels on the edge of the ZoR have all the required context.

After passing the input tile through the model, the ZoR within the prediction (without the halo) is copied to the output image being constructed in CPU memory.
Note, the network output is the same size as its input, $ZoR + 2 \times Halo$.
The ZoR needs to be cropped out from the prediction so only those pixels with full context are used to build the final result.
The halo provides the network with all the information it needs to make correct, exact predictions for the entirety of the output zone of responsibility.
Hence the name, since each ZoR is responsible for a specific zone of the output image.

This tile-based inference can be thought of as a series of forward passes, each computing a subregion (ZoR) of the feature maps that would be created while performing inference on the whole image in one pass.
In summary, each tile's feature maps are created (forward pass), its ZoR output extracted, and then the GPU memory is recycled for the next tile.
By building each ZoR result in a separate forward pass we can construct the network output within a fixed GPU memory footprint for arbitrarily large images.

\subsection{U-Net Case Study}

We use U-Net \citep{Ronneberger2015a} as a case study example of FCNs.
Nonetheless, the presented tiling concept applies to any FCN (without dynamic or variable receptive fields), just the specific numerical values will be different.

Note: for the purpose of brevity, we will use `up-conv' (as the U-Net paper does) to refer to fractionally strided convolutions with a stride of $1/2$ which doubles the feature map spatial resolution \citep{Dumoulin2018}.

\subsubsection{Determining The Halo}

The halo must be half the receptive field ($halo = \lceil receptive field / 2 \rceil$).
The general principle is to sum along the longest path through the network, the product of half the receptive field for each convolutional kernel and the stride that kernel has across the input image.
The stride a specific convolution kernel has across the input image is a combination of that kernel's stride with respect to its feature map and the downsampling factor between the input image size and the spatial size of that feature map.
The downsampling factor is determined by the number of spatial altering layers between the input image and a specific layer.

Let U-Net be described by an ordered sequence of convolutional layers $c={0, ..., N-1}$ with each layer being associated with a level $l_{c}$ and a square kernel $k_{c} \times k_{c}$.
For the network, $N$ defines the number of convolutional layers along the longest path from input to output.

Let us define the level $l_{c}$ of an encoder-decoder network architecture as the number of max-pool\footnote{Convolutions with a stride of 2 can also be used to halve the spatial size of the feature maps but they will affect the receptive field.} layers minus the number of up-conv layers between the input image and the current convolutional layer $c$ along the longest path through the network.
Levels start at 0; each max pool encountered along the longest path increases the level by 1 and each up-conv reduces the level by 1.


\subsubsection{General Halo Calculation}

The required halo can be calculated according to the Equation \ref{eq:halo} for a UNet type FCN architecture.
This equation can be considered a special case of the more general framework presented by \cite{araujo2019computing} for computing CNN receptive fields; however it enables an exploration of how each UNet element contributes to the receptive field.

\begin{equation}
Halo = \sum_{c=0}^{N-1} 2^{l_c} \lfloor \frac{k_c}{2} \rfloor
\label{eq:halo}
\end{equation}

The halo is a sum over every convolutional layer index $c$ from $0$ to $N-1$ encountered along the longest path from the input image to the output.
Equation \ref{eq:halo} has two terms.
The $2^{l_c}$ term is the number of pixels at the input image resolution that correspond to a single pixel within a feature map at level $l_c$.
Therefore, at level $l_c=4$ each pixel in the feature map equates to $2^4 = 16$ pixels at the input image resolution.
This $2^{l_c}$ term is multiplied by the second term $\lfloor \frac{k_c}{2} \rfloor$ which determines, for a given $c$, how many pixels of local context are required at that feature map resolution to perform the convolution.

\subsubsection{U-Net Configuration}

We have made two modifications to the published U-Net.

\begin{enumerate}
	\item Normalization: Batch normalization \citep{ioffe2015batch} was added after the activation function of each convolutional layer as it is current good practice in the CNN modeling community.
	\item Convolution Type: The convolutional padding scheme was changed to \texttt{SAME} from \texttt{VALID} as used in the original paper \citep{Ronneberger2015a}.
\end{enumerate} 

Batch normalization will not prevent numerically identical results compared to inferring the whole image in a single pass if correct inference procedures are followed, where the normalization statistics are frozen having been estimated during the training process.

The original U-Net paper uses \texttt{VALID} type convolutions\footnote{For an excellent review of convolutional arithmetic, including transposed convolutions (i.e., up-conv), see ``A guide to convolutional arithmetic for deep learning by Dumoulin and Visin" \citep{Dumoulin2018}.} which shrink the spatial size of the feature maps by 2 pixels for each layer \citep{Dumoulin2018}.
Switching to \texttt{SAME} type convolutions preserves feature map size.
See Appendix D for additional explanation.

There is one additional constraint on U-Net that needs to be mentioned.
Given the skip connections between the encoder and decoder elements for matching feature maps, we need to ensure that the tensors being concatenated together are the same size.
This can be restated as requiring the input size be divisible by the largest stride across the input image by any kernel.
For U-Net this stride is $16$ (derivation in Appendix D).
Another approach to ensuring consistent size between the encoder and decoder is to pad each up-conv layer to make its output larger and then crop to the target feature map size.
That is a less elegant solution than enforcing a tile size constraint and potentially padding the input image.


\subsubsection{Halo Calculation for U-Net}

The published U-Net (Figure 1 from \citep{Ronneberger2015a}) has one level per horizontal stripe of layers.
The input image enters on level $l_{c=0} = l_{c=1} = 0$.
The first max-pool layer halves the spatial resolution of the network, changing the level.
Convolution layers $c = \{2, 3\}$ after that first max-pool layer up to the next max-pool layer belong to level $l_{c=2}=l_{c=3} = 1$.
This continues through level 4, where the bottleneck of the U-Net model occurs.
In U-Net's Figure 1 \citep{Ronneberger2015a}, the bottleneck is the feature map at the bottom which occurs right before the first up-conv layer.
After the bottleneck, the level number decreases with each subsequent up-conv layer, until level $l_{N-1} = 0$ right before the output image is generated.

The halo computation in Equation \ref{eq:halo} can be simplified for U-Net as shown in Appendix A.
Appendix B shows a numerical example for computing the halo size using Equation \ref{eq:halo}.

Following Equation \ref{eq:halo} for U-Net results in a minimum required halo of 92 pixels in order to provide the network with all of the local context it needs to predict the outputs correctly.
This halo needs to be provided both before and after each spatial dimension and hence the input image to the network will need to be $2 \times 92 = 184$ pixels larger.
This value is exactly the number of pixels the original U-Net paper has the output being shrunk by to avoid using \texttt{SAME} convolutions; a 572 pixel input shrunk by 184 results in the 388 pixel output \citep{Ronneberger2015a}.
However, this runs afoul of our additional restriction on the U-Net input size, which requires images to be a multiple of 16.
So rounding up to the nearest multiple of 16 results in a halo of 96 pixels.
Since  one cannot just adjust the ZoR size to ensure $(ZoR + Halo) \% 16 = 0$ due to convolutional arithmetic, we must explore constraints on image partitioning.

\subsection{Constraints on Image Partitioning}

Our tile-based processing methodology operates on the principle of constructing the intermediate feature map representations within U-Net in a tile-based fashion, such that they are numerically identical to the whole image being passed through the network in a single pass.
Restated another way, the goal is to construct an input image partitioning scheme such that the zone of responsibility is building a spatial subregion of the feature maps that would exist if the whole image were passed through the network in a single pass.

\subsubsection{Stride Selection}
To properly construct this feature map subregion one cannot stride across the input image in a different manner than would be used to perform inference on the whole image.
The smallest feature map in U-Net is spatially $16 \times$ smaller than the input image.
Therefore, 16 pixels is the smallest offset one can have between two tile-based forward network passes while having both collaboratively build subregions of a single feature map representation.
Figure \ref{fig:offset} shows a simplified 1D example with a kernel of size 3 performing addition.
When two applications of the same kernel are offset by less than the size of the kernel they can produce different results.
For U-Net, each $16 \times 16$ pixel block in the input image becomes a single pixel in the lowest spatial resolution feature map.
A stride other than a multiple of 16 would result in subtly different feature maps because each feature map pixel was constructed from a different set of $16 \times 16$ input pixels.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figs/partitioning.png}
	\caption{A (left): Simplified 1D example of an addition kernel of size 3 being applied at an offset less than the kernel size, producing different results (compare top and bottom sums of 1, 2, and 3 or 2, 3, and 4).
		B (right): Simplified 1D example of reflection padding (reflected through dotted line) causing a different stride pattern across a set of pixels.
		The altered stride prevents the tile-based processing from collaboratively building subregions of a single feature map.}
	\label{fig:offset}
\end{figure}

This requirement means that we always need to start our tiling of the full image at the top left corner and stride across in a multiple of 16.
However, this does not directly answer the question as to why we cannot have a non-multiple of 16 halo value.

\subsubsection{Border Padding}
The limitation on the halo comes from the fact that if we have arbitrary halo values, we will need to use different padding schemes between the full image inference and the tile-based inference to handle the image edge effects.
Figure \ref{fig:offset} shows for a 1D case how reflection padding can (1) alter the stride across the full image which needs to be maintained as a multiple of 16 to collaboratively build subregions of a single feature map and (2) change the reflection padding required to have an input image whose spatial dimensions are a multiple of 16.
Reflection padding is preferred since it preserves image statistics locally.

\subsubsection{ZoR and Halo Constraints}

Both problems, (1) collaboratively building feature maps and (2) different full image edge reflection padding requirements disappear if both the zone of responsibility and the halo are multiples of 16.
Thus, we constrain the final values of ZoR and halo to be the closest higher multiple of the ratio $F$ between the image size $I$ and minimum feature map size (Equation \ref{eq:constraints}) where $F=16$ for the published U-Net, 
\begin{equation}
\begin{aligned}
F = \frac{ \min\{H_{I}, W_{I} \} }{ \min_{\forall l_{c}} \{ H_{l_{c}}, W_{l_{c}} \} } \\
Halo^{*} = F \lceil \frac{Halo}{F} \rceil \\
ZoR = F \lceil \frac{ZoR}{F} \rceil 
\end{aligned}
\label{eq:constraints}
\end{equation}
where $H_{I}$ and $W_{I}$ are the input image height and width dimensions, $Halo^{*}$ is the adjusted halo value to accommodate stride constraints, and $H_{l_{c}}$ and $W_{l_{c}}$ are the feature map height and width dimensions.

\section{Experimental Results}
\label{experimental-results}

\subsection{Data Set}
\label{dataset}

We used a publicly accessible data set acquired in phase contrast imaging modality and published in \citep{Bhadriraju2016}.
The data set consists of three collections, each with around 161 time-lapse images at roughly $\num{20000} \times \num{20000}$  pixels per stitched image frame with 2 bytes per pixel.


\subsection{Exact Tile-Based Inference Scheme}

Whether performing inference on the whole image in a single forward pass or using tile-based processing, the input image size needs to be a multiple of 16 as previously discussed Reflection padding is applied to the input image to enforce this size constraint before the image is decomposed into tiles.

Let us assume that we know how big an image we can fit into GPU memory, for example $1024 \times 1024$ pixels.
Additionally, given that we are using U-Net we know that the required halo is 96 pixels.
Then our zone of responsibility is $ZoR = 1024 - 2 \times Halo = 832$ pixels per spatial dimension.
Despite performing inference on $1024 \times 1024$ pixel tiles on the GPU per forward pass, the stride across the input image is 832 pixels because we need non-overlapping ZoR.
The edges of the full image do not require halo context to ensure identical results when compared with a single inference pass.
Intuitively, the true context is unknown since it's outside the existing image.

In the last row and column of tiles, there might not be enough pixels to fill out a full $1024 \times 1024$ tile.
However, because U-Net can alter its spatial size on demand, as long as the tile is a multiple of 16 a narrower (last column) or shorter (last row) tile can be used.


\subsection{Errors due to an Undersized Halo}

To experimentally confirm that our out-of-core image inference methodology does not impact the results we determined the largest image we could process on our GPU, performed the forward pass, and saved the resulting softmax output values as ground truth data.
We then process the same image using our tiling scheme with varying halo values.
We show that there are numerical differences (greater than floating point error) when using halo values less than 96.

Our U-Net model was trained to perform binary (foreground/background) segmentation of the phase contrast microscopy images.
The largest image on which we could perform inference given our GPU with $\SI{24}{\giga\byte}$ of memory is $3584 \times 3584$ pixels.
Therefore, we created 20 reference inference results by cropping out $K = 20$ random $3584 \times 3584$ subregions of the data set.
Tile-based out-of-core inference was performed for each of the 20 reference images using a tile size of 512 pixels (thereby meeting the multiple of 16 constraint) with halo values from 0 to 96 pixels in 16 pixel increments.

The tiling codebase seamlessly constructs the softmax output in CPU memory as if the whole image had been inferred in a single forward pass.
So our evaluation methodology consists of looking for differences in the output softmax produced by the reference forward pass ($R$) as well as the tile-based forward pass ($T$).
We used the following two metrics for evaluation: Root Mean Squared Error (RMSE, Equation \ref{eq:rmse}) of the softmax and Misclassification Error (ME, Equation \ref{eq:me}) of the resulting binary segmentation masks.
We also include Misclassification Error Rate (MER, Equation \ref{eq:mer}), the ME normalized by the number of pixels; and Relative Runtime, where compute time to perform inference is shown relative to the runtime without the tiling scheme.
This runtime highlights that there is a tradeoff to be made between the error introduced due to the out-of-core GPU inference and the compute overhead required to do so.
The MER metric can be multiplied by $100$ to compute the percent of pixels with errors due to the tiling scheme.
All metrics are averaged across the $K = 20$ reference images.

\begin{equation}
RMSE = \frac{1}{K} \sum_{i=1}^{K} \sqrt{ \frac{\sum_{i = 1}^{m} \sum_{j = 1}^{n} (R_{ij} - T_{ij})^2}{mn}}
\label{eq:rmse}
\end{equation}

\begin{equation}
ME = \frac{1}{K} \sum_{i=1}^{K} \left( \sum_{i = 1}^{m} \sum_{j = 1}^{n} [ R_{ij} \neq T_{ij} ] \right) 
% https://en.wikipedia.org/wiki/Iverson_bracket
\label{eq:me}
\end{equation}

\begin{equation}
MER = \frac{1}{K} \sum_{i=1}^{K} \left( \frac{\sum_{i = 1}^{m} \sum_{j = 1}^{n} [ R_{ij} \neq T_{ij} ]}{nm} \right) 
% https://en.wikipedia.org/wiki/Iverson_bracket
\label{eq:mer}
\end{equation}

The total inference error is a composite of model error (what is directly minimized by gradient descent during training) and tiling error.
We demonstrate a zero error contribution from tiling in the case of trained and untrained U-Net models (or minimum and maximum inference errors due to a model).

For the trained model, the error metrics are shown in Table \ref{tab:tile_size_512} with 512 pixel tiles \footnote{All results were generated on an Intel Xeon 4114 CPU with an NVIDIA Titan RTX GPU using Python 3.6 and TensorFlow 2.1.0 running on Ubuntu 18.04.}.
Once the required 96 pixel halo is met the RMSE falls into the range of floating point error and ME goes to zero.
Beyond the minimum required halo, all error metrics remain equivalent to the minimum halo.
The first row shows the data for the whole image being inferred without the tiling scheme.
The ME metric is especially informative, because when it is zero, the output segmentation results are identical regardless of whether the whole image was inferred in a single pass or it was decomposed into tiles.
Table \ref{tab:tile_size_512} highlights the engineering tradeoff that must be made, obtaining zero inference error requires $2.58 \times$ the wall clock runtime.
The MER with naive tailing is $\num{6.1e-4}$ or $\num{0.06} \%$.
Depending on your application, this level of error might be acceptable despite the potential for edge effects between the non-overlapping tiles.
One consideration: larger tile sizes are more compute efficient because the ratio of the ZoR area to the tile area increases.

\begin{table}
	\centering
	\caption{Error Metrics for Tile Size 512}
	\label{tab:tile_size_512}
	\begin{tabular}{r|r|r|r|r|r|r|r}
		TileSize & ZoR & Halo & RMSE    & ME & $\sigma$(ME) & MER & RelativeRuntime \\ 
		\hline
		3584 & n/a & n/a & 0.0 & 0.0 & 0.0 & 0.0 & 1.0 \\
		512 & 512 & 0 & $\num{1.11e-02}$ & 7773.4 & 1350 & $\num{6.1e-04}$ & 1.08 \\
		512 & 480 & 16 & $\num{6.35e-03}$ & 5455.4 & 1420 & $\num{4.2e-04}$ & 1.31 \\
		512 & 448 & 32 & $\num{3.29e-03}$ & 2372.2 & 670 & $\num{1.8e-04}$ & 1.36 \\
		512 & 416 & 48 & $\num{1.95e-03}$ & 1193.7 & 350 & $\num{9.3e-05}$ & 1.61 \\
		512 & 384 & 64 & $\num{7.79e-04}$ & 434.1 & 141 &$\num{3.4e-05}$ & 1.85 \\
		512 & 352 & 80 & $\num{1.50e-04}$ & 71.6 & 27 & $\num{5.6e-06}$ & 2.21 \\
		512 & 320 & 96 & $\num{4.17e-10}$ & 0.0 & 0.0 & 0.0 & 2.58 \\
	\end{tabular}
\end{table}

For the untrained model, results are shown in Table \ref{tab:tile_size_1024} in Appendix C with 1024 pixel tiles.
The results were generated using an untrained 4 class U-Net model, whose weights were left randomly initialized.
Additionally, the image data for that result was normally distributed random noise with $\mu = 0, \sigma = 1$.
The error coming from tile-based processing is zero once the required halo is met.


\subsection{Errors due to Violation of Partitioning Constraints}

To demonstrate how the inference results differ as a function of how the network strides across the input image we have constructed 32 overlapping, $2048 \times 2048$ pixel subregions of an image; each offset from the previous subregion start by 1 pixel.
So the first subregion is $[x_{st}, y_{st}, x_{end}, y_{end}] = [0, 0, 2048, 2048]$, while the second subregion is $[1, 0, 2049, 2048]$, and so on.
In order to compare the inference results without any edge effects confounding the results, we only compute RMSE (Equation \ref{eq:rmse}) of the softmax output within the area in common between all 32 images, inset by 96 pixels; $[128, 96, 1920, 1952]$.
The results are shown in Figure \ref{fig:stride-impact} where identical softmax outputs only happen when the offset is a multiple of 16.

\begin{figure}
	\centering
	\includegraphics[width=0.6\linewidth]{figs/stride_impact.png}
	\caption{Impact of the stride offset on the RMSE of the U-Net softmax output.}
	\label{fig:stride-impact}
\end{figure} 

\subsection{Application to a Fully Convolutional DenseNet}

Up to this point we have shown that our ZoR and halo tiling scheme produces error-free out-of-core semantic segmentation inference for arbitrarily large images when using the published U-Net architecture \citep{Ronneberger2015a}.
This section demonstrates the tiling scheme on a Fully Convolutional DenseNet configured for Semantic Segmentation \citep{Jegou2017}.
DenseNets \citep{Huang2017} replace stacked convolutional layers with densely connected blocks where each convolutional layer is connected to all previous convolutional layers in that block.
Jegou et al. \citep{Jegou2017} extended this original DenseNet idea to create a fully convolutional DenseNet based semantic segmentation architecture.

While the architecture of DenseNets significantly differs from U-Net, the model is still fully convolutional and thus our tiling scheme is applicable.
Following Equation \ref{eq:halo} for a FC-DenseNet-56 \citep{Jegou2017} model produces a required halo value of 377.
This is significantly higher than U-Net due to the architecture depth.
FC-DenseNet-56 also has a ratio between the input image size and the smallest feature map of $F = 32$.
Therefore the inference image sizes need to be a multiple of 32, not 16 like the original U-Net.
Thus, the computed 377 pixel halo is adjusted up to 384.

The error metrics for FC-DenseNet-56 as a function of halo are showing in Table \ref{tab:tile_size_1152} in Appendix C.
This numerical analysis relies on versions of the same 20 test images from the U-Net analysis, but cropped to $\num{2304} \times \num{2304}$, which was the largest image we were able to perform inference using FC-DenseNet-56 on our $\SI{24}{\giga\byte}$ GPU in a single forward pass.


\subsection{Halo Approximation via Effective Receptive Field}

The halo value required to achieve error free inference increases with the depth of the network.
For example, see Table \ref{tab:common_radii} which shows the theoretical halo values (computed using Equation \ref{eq:halo}) for a few common semantic segmentation architectures.
The deeper networks, like FC-DenseNet-103 require very large halo values to guarantee error free tile based inference.

\begin{table}
	\centering
	\caption{Theoretical Radii for Common Segmentation Architectures}
	\label{tab:common_radii}
	\begin{tabular}{r|r|r}
		Architecture & Halo &  \\ 
		\hline
		U-Net \citep{Ronneberger2015a} & 96 \\
		SegNet \citep{Badrinarayanan2015a} & 192 \\
		FCN-VGG16 \citep{Long2015} & 202 \\
		FC-DenseNet-56 \citep{Jegou2017} & 384 \\
		FC-DenseNet-67 \citep{Jegou2017} & 480 \\
		FC-DenseNet-103 \citep{Jegou2017} & 1120 \\
	\end{tabular}
\end{table}

Using the empirical effective receptive field estimation method outlined by Luo et al. \citep{Luo2016} which consists of setting the loss to 1 in the center of the image and then back propagating to the input image, we can automatically estimate the required halo.
For our trained U-Net \citep{Ronneberger2015a} this method produces an estimated halo of 96 pixels, which is exactly the theoretical halo.
This matches the data in Tables \ref{tab:tile_size_512} and \ref{tab:tile_size_1024} where the ME metric did not fall to zero until the theoretical halo was reached.
On the other hand, according to Table \ref{tab:tile_size_1152} for FC-DenseNet-56, there is no benefit to using a halo larger than 192 despite the theoretical required halo (receptive field) being much larger.
This is supported by the effective receptive field estimated halo of 160 pixels; just below the empirically discovered minimum halo of 192.
Thus, using the effective receptive field for estimating the required halo is not foolproof but it provides a good proxy for automatically reducing the tiling error.
The effective receptive field will always be less than or equal to the true networks potential receptive field because convolution kernel weights can learn to ignore information, but cannot increase the kernel size.


\section{Conclusions}
\label{conclusion}

This paper outlined a methodology for performing error-free segmentation inference of arbitrarily large images.
We documented the formulas for determining the tile-based inference scheme parameters.
We then demonstrated that the inference results are identical regardless of whether or not tiling was used.
These inference scheme parameters were related back to the theoretical and effective receptive fields of deep convolutional networks as previously studied in literature \citep{Luo2016}.
The empirical effective receptive field estimation methods of Luo et al. \citep{Luo2016} were used to provide a rough estimate of the inference tiling scheme parameters without requiring any knowledge of the architecture.
While we used U-Net and FC-DenseNets as example FCN models, these principles apply to any FCN model while being robust across different choices of tile size.

In this work we did not consider any FCN networks with dilated convolutions, which are known to increase the receptive field side of the network.
We will include this extension in future work as well as tradeoff evaluations of the relationships among relative runtime, GPU memory size and maximum tile size.


\section{Test Data and Source Code}
The test data and the Tensorflow v2.x source code are available from public URLs\footnote{
	\url{https://isg.nist.gov/deepzoomweb/data/stemcellpluripotency} \\
	\url{https://github.com/usnistgov/semantic-segmentation-unet/tree/ooc-inference}.}.
While the available codebase in theory supports arbitrarily large images, we made the choice at implementation time to load the whole image into memory before processing it through the network.
In practice this means the codebase is limited to inferencing images which fit into CPU memory.
However, using a file format which supports reading sub-sections of the whole image would support performing inference of disk-backed images which do not fit into CPU memory.

\section{Disclaimer}

Commercial products are identified in this document in order to specify the experimental procedure adequately.
Such identification is not intended to imply recommendation or endorsement by NIST, nor is it intended to imply that the products identified are necessarily the best available for the purpose.
Analysis performed [in part] on the NIST Enki HPC cluster.
Contribution of U.S. government not subject to copyright.

\clearpage

\appendix
\section{Derivation of Simplified Halo Formula}

Let us assume that in the entire U-Net architecture the kernel size is constant $k_{c} = k = const$ and each level has the same number of convolutional layers on both decoder and encoder sides $n_{l} = n = const$.
If these constraints are satisfied, then the general formula for determining halo can be simplified as follows:

\begin{equation}
\begin{aligned}
Halo = \sum_{c=1}^{N} 2^{l_c} \lfloor \frac{k_c}{2} \rfloor \\
= \lfloor \frac{k}{2} \rfloor \times \sum_{c=1}^{N} 2^{l_c} \\
= \lfloor \frac{k}{2} \rfloor \times ( 2 \times \sum_{m=0}^{M-1} (2^{m} \times n) + 2^{M} \times n )  \\
= \lfloor \frac{k}{2} \rfloor \times n \times ( 2 \times \frac{1 \times (1 - 2^M) }{ 1- 2} ) + 2^{M} )  \\
= \lfloor \frac{k}{2} \rfloor \times n \times (3 \times 2^{M} - 2) 
\end{aligned}
\label{eq:halo2}
\end{equation}

\noindent
where $M$ is the maximum U-Net level $M = \max_{\forall c}\{ l_{c} \}$ .

For U-Net architecture, the parameters are $k = 3$, $n=2$ and $M=4$, and the equation yields $Halo=92$.

For DenseNet architecture, the parameters are $k = 3$, $n=4$ and $M=5$, and the equation yields $Halo=376$.
This value differs by one from the value computed according to Equation 1 because the DenseNet has asymmetry between the first encoder layer with a kernel size $k = 3$ and the last decoder layer with a kernel size $k = 1$.


\section{Example U-Net Halo Calculation}

Following Equation \ref{eq:halo} for U-Net results in a required halo of 92 pixels in order to provide the network with all of the local context it needs to predict the outputs correctly.
With $k = 3$, $\lfloor \frac{k_c}{2} \rfloor$ reduces to $\lfloor \frac{3}{2} \rfloor = 1$.
The halo computation for U-Net thus reduces to a sum of $2^{l_c}$ terms for each convolutional layer encountered along the longest path from input to output as shown in Equation \ref{eq:unet-halo}.

\begin{equation}
Halo = \sum_{c=0}^{17} 2^{l_c}
\label{eq:unet-halo}
\end{equation}

By substituting the level numbers for each convolutional layer from 0 to 17 as shown in Equation \ref{eq:unet-halo-num}, one obtains the minimum halo value of 92 pixels.

\begin{equation}
\begin{small}
\begin{aligned} 
l_c = \{0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 3, 3, 2, 2, 1, 1, 0, 0\} \\
92 = 2^0 + 2^0 + 2^1 + 2^1 + 2^2 + 2^2 + 2^3 + ...
\end{aligned}
\end{small}
\label{eq:unet-halo-num}
\end{equation}

Similarly, according to Equation \ref{eq:halo2}, the calculation simplifies to:
\begin{equation}
\begin{aligned} 
M=\max_{\forall c} { l_c } = 4\\
k_c = k = 1  \\
n_l = n = 2 \\
92 = 1 \times 2 \times (3 \times 2^4 - 2)
\end{aligned}
\label{eq:unet-halo-num2}
\end{equation}


\section{Error Metrics}

\begin{table}[h!]
	\centering
	\caption{UnTrained U-Net Error Metrics for Tile Size 1024}
	\label{tab:tile_size_1024}
	\begin{tabular}{r|r|r|r|r|r|r}
		TileSize & ZoR & Halo & RMSE    & ME & MER & RelativeRuntime \\ 
		\hline
		3584 & n/a & n/a & 0.0 & 0.0 & 0.0 & 1.0 \\
		1024 & 1024 & 0 & $\num{2.36e-04}$ & 21856.9 & $\num{1.7e-03}$ & 1.08 \\
		1024 & 992 & 16 & $\num{1.05e-06}$ & 234.8 & $\num{1.8e-05}$ & 1.23 \\
		1024 & 960 & 32 & $\num{1.92e-07}$ & 49.7 & $\num{3.9e-06}$ & 1.30 \\
		1024 & 928 & 48 & $\num{5.47e-08}$ & 13.2 & $\num{1.0e-06}$ & 1.35 \\
		1024 & 896 & 64 & $\num{1.44e-08}$ & 2.8 & $\num{2.1e-07}$ & 1.31 \\
		1024 & 864 & 80 & $\num{5.87e-09}$ & 1.4 & $\num{1.1e-07}$ & 1.5 \\
		1024 & 832 & 96 & $\num{3.54e-10}$ & 0.0 & 0.0 & 1.58 \\
	\end{tabular}
\end{table}


\begin{table}[h!]
	\centering
	\caption{Error Metrics for FC-DenseNet Tile Size 1152}
	\label{tab:tile_size_1152}
	\begin{tabular}{r|r|r|r|r|r|r}
		TileSize & ZoR & Halo & RMSE    & ME & MER & RelativeRuntime \\ 
		\hline
		2304 & n/a & n/a & 0.0 & 0.0 & 0.0 & 1.0 \\
		1152 & 1152 & 0 & $\num{5.40e-03}$ & 821.5 & $\num{1.5e-04}$ & 1.15 \\
		1152 & 1088 & 32 & $\num{1.81e-03}$ & 376.1 & $\num{7.1e-05}$ & 1.42 \\
		1152 & 1024 & 64 & $\num{9.16e-04}$ & 168.0 & $\num{3.2e-05}$ & 1.54 \\
		1152 & 960 & 96 & $\num{3.36e-04}$ & 51.6 & $\num{9.7e-06}$ & 1.59 \\
		1152 & 896 & 128 & $\num{5.63e-05}$ & 5.3 & $\num{1.0e-06}$ & 1.67 \\
		1152 & 832 & 160 & $\num{4.97e-06}$ & 0.2 & $\num{4.7e-08}$ & 1.76 \\
		1152 & 768 & 192 & $\num{2.44e-07}$ & 0.0 & 0.0 & 2.32 \\
		1152 & 704 & 224 & $\num{1.92e-08}$ & 0.0 & 0.0 & 2.22 \\
		1152 & 640 & 256 & $\num{1.37e-08}$ & 0.0 & 0.0 & 2.33 \\
		1152 & 576 & 288 & $\num{1.44e-08}$ & 0.0 & 0.0 & 2.39 \\
		1152 & 512 & 320 & $\num{1.37e-08}$ & 0.0 & 0.0 & 2.52 \\
		1152 & 448 & 352 & $\num{1.45e-08}$ & 0.0 & 0.0 & 4.65 \\
		1152 & 384 & 384 & $\num{1.37e-08}$ & 0.0 & 0.0 & 5.89 \\
	\end{tabular}
\end{table}

\section{\texttt{SAME}  vs \texttt{VALID} Convolution Padding}

The original U-Net paper uses \texttt{VALID} type convolutions which shrink the spatial size of the feature maps by 2 pixels for each layer \cite{Dumoulin2018}\footnote{For an excellent review of convolutional arithmetic, including transposed convolutions (i.e., up-conv), see "A guide to convolutional arithmetic for deep learning by Dumoulin and Visin" \cite{Dumoulin2018}.}.
Figure 2.1 from Dumoulin and Visin "A guide to convolutional arithmetic for deep learning" \cite{Dumoulin2018} shows an illustration showing why \texttt{VALID} causes the feature maps to shrink.
The effect of \texttt{VALID} convolutions can also be seen in the first layer of the original U-Net where the input image size of $572 \times 572$ pixels shrinks to $570 \times 570$ \cite{Ronneberger2015a}.
Switching to \texttt{SAME} type convolutions requires that within each convolutional layer zero padding is applied to each feature map to ensure the output has the same spatial size as the input.
Figure 2.3 from Dumoulin and Visin \cite{Dumoulin2018} shows an illustration where a $ 5 \times 5$ input remains $5 \times 5$ after the convolution is applied.
While \texttt{VALID} type convolutions avoid the negative effects of the zero padding within \texttt{SAME} type convolutions, which can affect the results as outlined by Huang et al. \cite{Huang2019a}, it is conceptually simpler to have input and output images of the same size.
Additionally, our tiling scheme overcomes all negative effects that zero padding can introduce, justifying the choice of \texttt{SAME} type convolutions.

The change to \texttt{SAME} type convolutions introduces an additional constraint on U-Net that needs to be mentioned.
Given the skip connections between the encoder and decoder elements for matching feature maps, we need to ensure that the tensors being concatenated together are the same size.
This can be restated as requiring the input size be divisible by the largest stride across the input image by any kernel.
Another approach to ensuring consistent size between the encoder and decoder is to pad each up-conv layer to make its output larger and then crop to the target feature map size.
We feel that is a less elegant solution than enforcing a tile size constraint and potentially padding the input image.

The feature map at the bottleneck of U-Net is spatially $16 \times$ smaller than the input image.
Therefore, the input to U-Net needs to be divisible by $16$.
As we go deeper into a network we trade spatial resolution for feature depth.
Given a $512 \times 512$ pixel input image, the bottleneck shape will be $N \times 1024 \times 32 \times 32$ (assuming \texttt{NCHW}\footnote{NCHW Tensor dimension ordering: N (batch size), Channels, Height, Width} dimension ordering with unknown batch size).
Thus the input image height divided by the bottleneck feature map height is $\frac{512}{32} = 16$.
However, if the input image is $500 \times 500$ pixels, the bottleneck would be (in theory) $N \times 1024 \times 31.25 \times 31.25$.
When there are not enough input pixels in a feature map to perform the $2 \times 2$ max pooling, the output feature map size is the floor of the input size divided by $2$.
Thus, for an input image of $500 \times 500$ pixels the feature map heights after each max pooling layer in the encoder are: [$500, 250, 125, 62, 31$].
Now following the up-conv layers through the decoder, each of which doubles the spatial resolution, we end up with the following feature map heights: [$31, 62, 124, 248, 496$].
This results in a different feature map spatial size at the third level; encoder 125, decoder 124.
If the input image size is a multiple of $16$ this mismatch cannot happen.

\vspace{20pt}


%\noindent{\textbf{Acknowledgments}}\\
%\noindent The authors would like to acknowledge...\\
%%% This section is not required

\renewcommand{\bibname}{References}


{
	\footnotesize
\bibliographystyle{jresnist}
\bibliography{ooc-inference}} 

%% Please use the jresnist BibTeX style when compiling bibliography, or follow the instructions on tinyurl.com/jresnist to format your .bib / .bbl file appropriately. %%

\vspace{20pt} 

\noindent\textit{\textbf{About the authors:}}

\noindent\textit{Michael Majurski is a computer scientist in the Information Systems Group of the Software and Systems Division of the Information Technology Lab at NIST.
	His primary field of research is computer vision, specifically the processing and analysis of scientific images using traditional computer vision, machine learning, and deep learning based methods.
	Building CNNs from small domain-specific datasets is particular specialty.}

\noindent\textit{Dr. Peter Bajcsy has been a member of the Information Systems Group in the Information Technology Laboratory at the National Institute of Standards and Technology (NIST) since 2011.
	Peter has been leading a project focusing on the application of computational science in biological metrology.
	Peter’s area of research is large-scale image-based analyses using machine learning, computer vision, and pattern recognition techniques.}

\noindent\textit{The National Institute of Standards and Technology is an agency of the U.S. Department of Commerce.} \\
%% The last sentence here is required


\end{document}


@article{Mnih2013,
	abstract = {Information extracted from aerial photographs has found applications in a wide range of areas including urban planning, crop and forest management, disaster relief, and climate modeling. At present, much of the extraction is still performed by human experts, making the process slow, costly, and error prone. The goal of this thesis is to develop methods for automatically extracting the locations of objects such as roads, buildings, and trees directly from aerial images. We investigate the use of machine learning methods trained on aligned aerial images and possibly outdated maps for labeling the pixels of an aerial image with se- mantic labels. We show how deep neural networks implemented on modern GPUs can be used to efficiently learn highly discriminative image features. We then introduce new loss functions for training neural networks that are partially robust to incom- plete and poorly registered target maps. Finally, we propose two ways of improving the predictions of our system by introducing structure into the outputs of the neural networks. We evaluate our system on the largest and most-challenging road and building detection datasets considered in the literature and show that it works reliably under a wide variety of conditions. Furthermore, we are releasing the first large-scale road and building detection datasets to the public in order to facilitate future comparisons with other methods.},
	author = {Mnih, Volodymyr},
	file = {:Users/mmajursk/Documents/Mendeley Desktop/Mnih{\_}Volodymyr{\_}PhD{\_}Thesis.pdf:pdf},
	isbn = {9780494721308},
	journal = {PhD Thesis},
	mendeley-groups = {{\_}ToRead},
	pages = {109},
	title = {{Machine Learning for Aerial Image Labeling}},
	year = {2013}
}


@article{araujo2019computing,
	author = {Araujo, Andr√© and Norris, Wade and Sim, Jack},
	title = {Computing Receptive Fields of Convolutional Neural Networks},
	journal = {Distill},
	year = {2019},
	note = {https://distill.pub/2019/computing-receptive-fields},
	doi = {10.23915/distill.00021}
}

@inproceedings{Wang2018c,
	abstract = {Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.},
	archivePrefix = {arXiv},
	arxivId = {1711.07971},
	author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2018.00813},
	eprint = {1711.07971},
	file = {:Users/mmajursk/Documents/Mendeley Desktop/1711.07971.pdf:pdf},
	isbn = {9781538664209},
	issn = {10636919},
	mendeley-groups = {{\_}ToRead,Deep-Learning},
	title = {{Non-local Neural Networks}},
	year = {2018}
}

@article{Dai2017,
	abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/msracver/Deformable-ConvNets.},
	archivePrefix = {arXiv},
	arxivId = {1703.06211},
	author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
	doi = {10.1109/ICCV.2017.89},
	eprint = {1703.06211},
	file = {:Users/mmajursk/Documents/Mendeley Desktop/1703.06211.pdf:pdf},
	isbn = {9781538610329},
	issn = {15505499},
	journal = {Proceedings of the IEEE International Conference on Computer Vision},
	mendeley-groups = {{\_}ToRead,Deep-Learning},
	pages = {764--773},
	title = {{Deformable Convolutional Networks}},
	volume = {2017-Octob},
	year = {2017}
}

@inproceedings{Ramachandran2019b,
	abstract = {Convolutions are a fundamental building block of modern computer vision systems. Recent approaches have argued for going beyond convolutions in order to capture long-range dependencies. These efforts focus on augmenting convolutional models with content-based interactions, such as self-attention and non-local means, to achieve gains on a number of vision tasks. The natural question that arises is whether attention can be a stand-alone primitive for vision models instead of serving as just an augmentation on top of convolutions. In developing and testing a pure self-attention vision model, we verify that self-attention can indeed be an effective stand-alone layer. A simple procedure of replacing all instances of spatial convolutions with a form of self-attention applied to ResNet model produces a fully self-attentional model that outperforms the baseline on ImageNet classification with 12{\%} fewer FLOPS and 29{\%} fewer parameters. On COCO object detection, a pure self-attention model matches the mAP of a baseline RetinaNet while having 39{\%} fewer FLOPS and 34{\%} fewer parameters. Detailed ablation studies demonstrate that self-attention is especially impactful when used in later layers. These results establish that stand-alone self-attention is an important addition to the vision practitioner's toolbox.},
	archivePrefix = {arXiv},
	arxivId = {1906.05909},
	author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jonathon},
	booktitle = {Neural Information Processing Systems},
	eprint = {1906.05909},
	file = {:Users/mmajursk/Documents/Mendeley Desktop/8302-stand-alone-self-attention-in-vision-models.pdf:pdf},
	mendeley-groups = {{\_}ToRead},
	title = {{Stand-Alone Self-Attention in Vision Models}},
	url = {http://arxiv.org/abs/1906.05909},
	year = {2019}
}

@article{Hu2018,
	abstract = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the 'Squeeze-and-Excitation' (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251{\%}, achieving a {\~{}}25{\%} relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.},
	archivePrefix = {arXiv},
	arxivId = {1709.01507},
	author = {Hu, Jie and Shen, Li and Sun, Gang},
	doi = {10.1109/CVPR.2018.00745},
	eprint = {1709.01507},
	file = {:Users/mmajursk/Documents/Mendeley Desktop/1709.01507.pdf:pdf},
	isbn = {9781538664209},
	issn = {10636919},
	journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	mendeley-groups = {{\_}ToRead},
	pages = {7132--7141},
	title = {{Squeeze-and-Excitation Networks}},
	year = {2018}
}



@article{Saito2016,
	abstract = {An automatic system to extract terrestrial objects from aerial imagery has many applications in a wide range of areas. However, in general, this task has been performed by human experts manually, so that it is very costly and time consuming. There have been many attempts at automating this task, but many of the existing works are based on class-specific features and classifiers. In this article, the authors propose a convolutional neural network (CNN)-based building and road extraction system. This takes raw pixel values in aerial imagery as input and outputs predicted three-channel label images (building-road-background). Using CNNs, both feature extractors and classifiers are automatically constructed. The authors propose a new technique to train a single CNN efficiently for extracting multiple kinds of objects simultaneously. Finally, they show that the proposed technique improves the prediction performance and surpasses state-of-the-art results tested on a publicly available aerial imagery dataset.},
	author = {Saito, Shunta and Yamashita, Takayoshi and Aoki, Yoshimitsu},
	doi = {10.2352/ISSN.2470-1173.2016.10.ROBVIS-392},
	file = {:Users/mmajursk/Documents/Mendeley Desktop/document(2).pdf:pdf},
	issn = {24701173},
	journal = {IS and T International Symposium on Electronic Imaging Science and Technology},
	mendeley-groups = {{\_}ToRead},
	number = {1},
	pages = {1--9},
	title = {{Multiple object extraction from aerial imagery with convolutional neural networks}},
	volume = {60},
	year = {2016}
}


@article{Luo2016,
	abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field, and show that it both has a Gaussian distribution and only occupies a fraction of the full theoretical receptive field. We analyze the effective receptive field in several architecture designs, and the effect of nonlinear activations, dropout, sub-sampling and skip connections on it. This leads to suggestions for ways to address its tendency to be too small.},
	archivePrefix = {arXiv},
	arxivId = {1701.04128},
	author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
	eprint = {1701.04128},
	file = {:home/mmajursk/Documents/Mendeley/1701.04128.pdf:pdf},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {Out-of-Core-Inference},
	number = {Nips},
	pages = {4905--4913},
	title = {{Understanding the effective receptive field in deep convolutional neural networks}},
	year = {2016}
}

@inproceedings{Lin2017a,
	author = {Lin, Tsung Yi and Doll{\'{a}}r, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	booktitle = {Computer Vision and Pattern Recognition},
	file = {:Users/mmajursk/Documents/Mendeley Desktop/Lin et al. - 2017 - Feature Pyramid Networks for Object Detection.pdf:pdf},
	mendeley-groups = {Deep-Learning},
	title = {{Feature Pyramid Networks for Object Detection}},
	year = {2017}
}

@article{Jaderberg2015,
	abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
	archivePrefix = {arXiv},
	arxivId = {1506.02025},
	author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
	eprint = {1506.02025},
	file = {:Users/mmajursk/Documents/Mendeley Desktop/1506.02025.pdf:pdf},
	issn = {10495258},
	journal = {Advances in Neural Information Processing Systems},
	mendeley-groups = {{\_}ToRead},
	pages = {2017--2025},
	title = {{Spatial transformer networks}},
	volume = {2015-January},
	year = {2015}
}



@article{Pleiss2017,
	abstract = {The DenseNet architecture is highly computationally efficient as a result of feature reuse. However, a naive DenseNet implementation can require a significant amount of GPU memory: If not properly managed, pre-activation batch normalization and contiguous convolution operations can produce feature maps that grow quadratically with network depth. In this technical report, we introduce strategies to reduce the memory consumption of DenseNets during training. By strategically using shared memory allocations, we reduce the memory cost for storing feature maps from quadratic to linear. Without the GPU memory bottleneck, it is now possible to train extremely deep DenseNets. Networks with 14M parameters can be trained on a single GPU, up from 4M. A 264-layer DenseNet (73M parameters), which previously would have been infeasible to train, can now be trained on a single workstation with 8 NVIDIA Tesla M40 GPUs. On the ImageNet ILSVRC classification dataset, this large DenseNet obtains a state-of-the-art single-crop top-1 error of 20.26{\%}.},
	archivePrefix = {arXiv},
	arxivId = {1707.06990},
	author = {Pleiss, Geoff and Chen, Danlu and Huang, Gao and Li, Tongcheng and van der Maaten, Laurens and Weinberger, Kilian Q.},
	eprint = {1707.06990},
	file = {:home/mmajursk/Documents/Mendeley/1707.06990.pdf:pdf},
	mendeley-groups = {Deep-Learning},
	title = {{Memory-Efficient Implementation of DenseNets}},
	url = {http://arxiv.org/abs/1707.06990},
	year = {2017}
}

@article{Blattner2017,
	abstract = {Designing applications for scalability is key to improving their performance in hybrid and cluster computing. Scheduling code to utilize parallelism is difficult, particularly when dealing with data dependencies, memory management, data motion, and processor occupancy. The Hybrid Task Graph Scheduler (HTGS) improves programmer productivity when implementing hybrid workflows for multi-core and multi-GPU systems. The Hybrid Task Graph Scheduler (HTGS) is an abstract execution model, framework, and API that increases programmer productivity when implementing hybrid workflows for such systems. HTGS manages dependencies between tasks, represents CPU and GPU memories independently, overlaps computations with disk I/O and memory transfers, keeps multiple GPUs occupied, and uses all available compute resources. Through these abstractions, data motion and memory are explicit; this makes data locality decisions more accessible. To demonstrate the HTGS application program interface (API), we present implementations of two example algorithms: (1) a matrix multiplication that shows how easily task graphs can be used; and (2) a hybrid implementation of microscopy image stitching that reduces code size by ‚âà 43{\%} compared to a manually coded hybrid workflow implementation and showcases the minimal overhead of task graphs in HTGS. Both of the HTGS-based implementations show good performance. In image stitching the HTGS implementation achieves similar performance to the hybrid workflow implementation. Matrix multiplication with HTGS achieves 1.3x and 1.8x speedup over the multi-threaded OpenBLAS library for 16k √ó 16k and 32k √ó 32k size matrices, respectively.},
	author = {Blattner, Timothy and Keyrouz, Walid and Bhattacharyya, Shuvra S. and Halem, Milton and Brady, Mary},
	doi = {10.1007/s11265-017-1262-6},
	file = {:Users/mmajursk/Documents/Mendeley Desktop/Blattner2017{\_}Article{\_}AHybridTaskGraphSchedulerForHi.pdf:pdf},
	issn = {19398115},
	journal = {Journal of Signal Processing Systems},
	keywords = {Dataflow,Heterogeneous architectures,Hybrid workflows,Image processing,Matrix multiplication,Task graph},
	mendeley-groups = {{\_}ToRead},
	number = {3},
	pages = {457--467},
	publisher = {Journal of Signal Processing Systems},
	title = {{A Hybrid Task Graph Scheduler for High Performance Image Processing Workflows}},
	volume = {89},
	year = {2017}
}

@inproceedings{Huang2017,
	abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
	archivePrefix = {arXiv},
	arxivId = {1608.06993},
	author = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q.},
	booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
	doi = {10.1109/CVPR.2017.243},
	eprint = {1608.06993},
	file = {:home/mmajursk/Documents/Mendeley/Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:pdf},
	isbn = {9781538604571},
	mendeley-groups = {Deep-Learning},
	month = {aug},
	pages = {2261--2269},
	title = {{Densely connected convolutional networks}},
	url = {http://arxiv.org/abs/1608.06993},
	volume = {2017-Janua},
	year = {2017}
}


@article{Jegou2017,
	abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py},
	archivePrefix = {arXiv},
	arxivId = {1611.09326},
	author = {Jegou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
	doi = {10.1109/CVPRW.2017.156},
	eprint = {1611.09326},
	file = {:home/mmajursk/Documents/Mendeley/1611.09326.pdf:pdf},
	isbn = {9781538607336},
	issn = {21607516},
	journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
	mendeley-groups = {Deep-Learning},
	pages = {1175--1183},
	pmid = {19244017},
	title = {{The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation}},
	volume = {2017-July},
	year = {2017}
}

@article{Russakovsky2015,
	abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the five years of the challenge, and propose future directions and improvements.},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	doi = {10.1007/s11263-015-0816-y},
	issn = {15731405},
	journal = {International Journal of Computer Vision},
	keywords = {Benchmark,Dataset,Large-scale,Object detection,Object recognition},
	mendeley-groups = {Out-of-Core-Inference},
	pages = {211--252},
	publisher = {Springer US},
	title = {{ImageNet Large Scale Visual Recognition Challenge}},
	url = {http://dx.doi.org/10.1007/s11263-015-0816-y},
	volume = {115},
	year = {2015}
}
@inproceedings{Badrinarayanan2015a,
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	archivePrefix = {arXiv},
	arxivId = {1511.00561},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	booktitle = {IEEE transactions on pattern analysis and machine intelligence},
	doi = {10.1109/TPAMI.2016.2644615},
	eprint = {1511.00561},
	isbn = {9783319464879},
	issn = {0162-8828},
	mendeley-groups = {Deep-Learning,Out-of-Core-Inference},
	pmid = {28060704},
	title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
	url = {http://arxiv.org/abs/1511.00561},
	year = {2017}
}
@article{Bhadriraju2016,
	abstract = {Identification and quantification of the characteristics of stem cell preparations is critical for understanding stem cell biology and for the development and manufacturing of stem cell based therapies. We have developed image analysis and visualization software that allows effective use of time-lapse microscopy to provide spatial and dynamic information from large numbers of human embryonic stem cell colonies. To achieve statistically relevant sampling, we examined {\textgreater} 680 colonies from 3 different preparations of cells over 5 days each, generating a total experimental dataset of 0.9 terabyte (TB). The 0.5 Giga-pixel images at each time point were represented by multi-resolution pyramids and visualized using the Deep Zoom Javascript library extended to support viewing Giga-pixel images over time and extracting data on individual colonies. We present a methodology that enables quantification of variations in nominally-identical preparations and between colonies, correlation of colony characteristics with Oct4 expression, and identification of rare events.},
	author = {Bhadriraju, Kiran and Halter, Michael and Amelot, Julien and Bajcsy, Peter and Chalfoun, Joe and Vandecreme, Antoine and Mallon, Barbara S. and Park, Kye-yoon and Sista, Subhash and Elliott, John T. and Plant, Anne L.},
	doi = {10.1016/j.scr.2016.05.012},
	issn = {18735061},
	journal = {Stem Cell Research},
	mendeley-groups = {Terascale-Image-Analysis-Reliability},
	number = {1},
	pages = {122--129},
	pmid = {27286574},
	title = {{Large-scale time-lapse microscopy of Oct4 expression in human embryonic stem cell colonies}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1873506116300502},
	volume = {17},
	year = {2016}
}
@inproceedings{Ronneberger2015a,
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	archivePrefix = {arXiv},
	arxivId = {1505.04597},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	booktitle = {International Conference on Medical image computing and computer-assisted intervention},
	eprint = {1505.04597},
	isbn = {978-3-319-24573-7},
	issn = {16113349},
	mendeley-groups = {Deep-Learning,Out-of-Core-Inference},
	month = {may},
	pages = {234--241},
	pmid = {23285570},
	title = {{U-Net: Convolutional Networks for Biomedical Image Segmentation}},
	year = {2015}
}
@misc{Bardakoff2019,
	author       = {Alexandre Bardakoff},
	title        = {Fast Image (FI) : A High-Performance Accessor for Processing Gigapixel Images},
	month        = Aug,
	year         = 2019,
	url          = {https://github.com/usnistgov/FastImage}
}
@inproceedings{Long2015,
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
	archivePrefix = {arXiv},
	arxivId = {1411.4038},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	doi = {10.1109/CVPR.2015.7298965},
	eprint = {1411.4038},
	isbn = {9781467369640},
	issn = {10636919},
	mendeley-groups = {Deep-Learning,Out-of-Core-Inference},
	pmid = {16190471},
	title = {{Fully convolutional networks for semantic segmentation}},
	year = {2015}
}
@article{Sherrah2016,
	title={Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery},
	author={Sherrah, Jamie},
	journal={arXiv preprint arXiv:1606.02585},
	year={2016}
}
@inproceedings{Lin2019,
	abstract = {Lymph node metastasis is one of the most important indicators in breast cancer diagnosis, that is traditionally observed under the microscope by pathologists. In recent years, with the dramatic advance of high-throughput scanning and deep learning technology, automatic analysis of histology from wholeslide images has received a wealth of interest in the field of medical image computing, which aims to alleviate pathologists' workload and simultaneously reduce misdiagnosis rate. However, automatic detection of lymph node metastases from whole-slide images remains a key challenge because such images are typically very large, where they can often be multiple gigabytes in size. Also, the presence of hard mimics may result in a large number of false positives. In this paper, we propose a novel method with anchor layers for model conversion, which not only leverages the efficiency of fully convolutional architectures to meet the speed requirement in clinical practice, but also densely scans the wholeslide image to achieve accurate predictions on both micro- and macro-metastases. Incorporating the strategies of asynchronous sample prefetching and hard negative mining, the network can be effectively trained. The efficacy of our method are corroborated on the benchmark dataset of 2016 Camelyon Grand Challenge. Our method achieved significant improvements in comparison with the state-of-the-art methods on tumour localization accuracy with a much faster speed and even surpassed human performance on both challenge tasks.;},
	author = {Lin, Huangjing and Chen, Hao and Graham, Simon and Dou, Qi and Rajpoot, Nasir and Heng, Pheng-Ann},
	booktitle = {IEEE Transactions on Medical Imaging},
	doi = {10.1109/tmi.2019.2891305},
	issn = {0278-0062},
	mendeley-groups = {Out-of-Core-Inference},
	pages = {1948--1958},
	title = {{Fast ScanNet: Fast and Dense Analysis of Multi-Gigapixel Whole-Slide Images for Cancer Metastasis Detection}},
	volume = {38},
	year = {2019}
}
@inproceedings{Volpi2017a,
	abstract = {Semantic labeling (or pixel-level land-cover classification) in ultra-high resolution imagery ({\textless} 10cm) requires statistical models able to learn high level concepts from spatial data, with large appearance variations. Convolutional Neural Networks (CNNs) achieve this goal by learning discriminatively a hierarchy of representations of increasing abstraction. In this paper we present a CNN-based system relying on an downsample-then-upsample architecture. Specifically, it first learns a rough spatial map of high-level representations by means of convolutions and then learns to upsample them back to the original resolution by deconvolutions. By doing so, the CNN learns to densely label every pixel at the original resolution of the image. This results in many advantages, including i) state-of-the-art numerical accuracy, ii) improved geometric accuracy of predictions and iii) high efficiency at inference time. We test the proposed system on the Vaihingen and Potsdam sub-decimeter resolution datasets, involving semantic labeling of aerial images of 9cm and 5cm resolution, respectively. These datasets are composed by many large and fully annotated tiles allowing an unbiased evaluation of models making use of spatial information. We do so by comparing two standard CNN architectures to the proposed one: standard patch classification, prediction of local label patches by employing only convolutions and full patch labeling by employing deconvolutions. All the systems compare favorably or outperform a state-of-the-art baseline relying on superpixels and powerful appearance descriptors. The proposed full patch labeling CNN outperforms these models by a large margin, also showing a very appealing inference time.},
	author = {Volpi, Michele and Tuia, Devis},
	booktitle = {IEEE Transactions on Geoscience and Remote Sensing},
	doi = {10.1109/TGRS.2016.2616585},
	issn = {01962892},
	keywords = {Aerial images,classification,convolutional neural networks (CNNs),deconvolution networks,deep learning,semantic labeling,subdecimeter resolution},
	mendeley-groups = {Out-of-Core-Inference},
	pages = {881--893},
	publisher = {IEEE},
	title = {{Dense semantic labeling of subdecimeter resolution images with convolutional neural networks}},
	volume = {55},
	year = {2016}
}
@inproceedings{Maggiori2016,
	abstract = {We propose a convolutional neural network (CNN) model for remote sensing image classification. Using CNNs provides us with a means of learning contextual features for large-scale image labeling. Our network consists of four stacked convolutional layers that downsample the image and extract relevant features. On top of these, a deconvolutional layer upsamples the data back to the initial resolution, producing a final dense image labeling. Contrary to previous frameworks, our network contains only convolution and deconvolution operations. Experiments on aerial images show that our network produces more accurate classifications in lower computational time.},
	author = {Maggiori, Emmanuel and Tarabalka, Yuliya and Charpiat, Guillaume and Alliez, Pierre},
	booktitle = {International Geoscience and Remote Sensing Symposium (IGARSS)},
	doi = {10.1109/IGARSS.2016.7730322},
	isbn = {9781509033324},
	keywords = {Remote sensing images,classification,convolutional neural networks,deep learning},
	mendeley-groups = {Out-of-Core-Inference},
	publisher = {IEEE},
	title = {{Fully convolutional neural networks for remote sensing image classification}},
	year = {2016}
}
@article{Huang2019a,
	title={Tiling and Stitching Segmentation Output for Remote Sensing: Basic Challenges and Recommendations},
	author={Huang, Bohao and Reichman, Daniel and Collins, Leslie M and Bradbury, Kyle and Malof, Jordan M},
	journal={arXiv preprint arXiv:1805.12219},
	year={2019}
}
@article{Iglovikov2017,
	title={Satellite imagery feature detection using deep convolutional neural network: A kaggle competition},
	author={Iglovikov, Vladimir and Mushinskiy, Sergey and Osin, Vladimir},
	journal={arXiv preprint arXiv:1706.06169},
	year={2017}
}
@article{ioffe2015batch,
	title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
	author={Ioffe, Sergey and Szegedy, Christian},
	journal={arXiv preprint arXiv:1502.03167},
	year={2015}
}
@article{Dumoulin2018,
	title={A guide to convolution arithmetic for deep learning},
	author={Dumoulin, Vincent and Visin, Francesco},
	journal={arXiv preprint arXiv:1603.07285},
	year={2016}
}
@article{lecun2015deep,
	title={Deep learning},
	author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	journal={nature},
	volume={521},
	number={7553},
	pages={436},
	year={2015},
	publisher={Nature Publishing Group}
}
@article{Sermanet2013,
	title={Overfeat: Integrated recognition, localization and detection using convolutional networks},
	author={Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Micha{\"e}l and Fergus, Rob and LeCun, Yann},
	journal={arXiv preprint arXiv:1312.6229},
	year={2013}
}
@inproceedings{VanEtten2019,
	abstract = {Detecting small objects over large areas remains a significant challenge in satellite imagery analytics. Among the challenges is the sheer number of pixels and geographical extent per image: a single DigitalGlobe satellite image encompasses over 64 km2 and over 250 million pixels. Another challenge is that objects of interest are often minuscule ({\~{}}pixels in extent even for the highest resolution imagery), which complicates traditional computer vision techniques. To address these issues, we propose a pipeline (SIMRDWN) that evaluates satellite images of arbitrarily large size at native resolution at a rate of {\textgreater} 0.2 km2/s. Building upon the tensorflow object detection API paper, this pipeline offers a unified approach to multiple object detection frameworks that can run inference on images of arbitrary size. The SIMRDWN pipeline includes a modified version of YOLO (known as YOLT), along with the models of the tensorflow object detection API: SSD, Faster R-CNN, and R-FCN. The proposed approach allows comparison of the performance of these four frameworks, and can rapidly detect objects of vastly different scales with relatively little training data over multiple sensors. For objects of very different scales (e.g. airplanes versus airports) we find that using two different detectors at different scales is very effective with negligible runtime cost.We evaluate large test images at native resolution and find mAP scores of 0.2 to 0.8 for vehicle localization, with the YOLT architecture achieving both the highest mAP and fastest inference speed.},
	author = {{Van Etten}, Adam},
	booktitle = {2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
	doi = {10.1109/WACV.2019.00083},
	isbn = {9781728119755},
	mendeley-groups = {Out-of-Core-Inference},
	pages = {735--743},
	publisher = {IEEE},
	title = {{Satellite imagery multiscale rapid detection with windowed networks}},
	year = {2019}
}



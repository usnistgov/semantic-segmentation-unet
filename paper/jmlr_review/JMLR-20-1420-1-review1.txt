The author identified the need to design a methodology for arbitrarily large image inference on memory constrained GPU for image segmentation where the loss of information due to image resizing is not acceptable. To overcome the GPU memory size constraint without numerically impacting the result, the authors investigate error-free out-of-core semantic segmentation inference of arbitrarily large images using fully convolutional neural networks. The key of the  approach is to select a tile size which will fit into GPU memory with a halo border of half the network receptive field. Then, stride across the image by the tile size without the halo.  

The novelty of the work is about determining the tile size and stride and validating the formula on U-Net and FC-DenseNet architectures. They quantified the errors due to tiling configurations which do not satisfy the constraints, and explored using architecture effective receptive fields to estimate the tiling parameters.

The authors provide empirical analysis and formulation about how to determine the tile size and halo size for keeping the same inference result, which is practically useful and appreciated. However, the significance of the contribution is not strong enough and it is not immediately clear why the method is necessary and how much it advances with naive methods. The topic of work is more about image processing with neural networks rather than “learning”. The problem seems very specific to U-Nets with the assumption that readers are familiar with its details, such as SAME and VALID convolutions in U-Net. I would suggest the authors to make the motivation clearer to emphasize the difficulties and the errors of using a traditional tile based approach. The author could give specific examples of a large image inference application, how large the image could be, how much memory it would actually cost and why traditional tiled based methods would fail.
